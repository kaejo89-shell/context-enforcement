[rank: 0] Global seed set to 1376
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[rank: 0] Global seed set to 1376
[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 768.33it/s]
['trained-models/bart-base-context-enhance-form1/checkpoint-46761', 'trained-models/bart-base-context-enhance-form1/checkpoint-29757', 'trained-models/bart-base-context-enhance-form1/checkpoint-51012']
Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 1024). Running this sequence through the model will result in indexing errors
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.4.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._wlc.bias', 'encoder.layers.4.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.weight', 'encoder.layers.2.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.4.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.4.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.2.context_enforcer._wrc.weight', 'encoder.layers.4.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wlc.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Performing inference using checkpoint-46761
Generating for the context length: 100
Done
{'bleu': 0.10584273430267338, 'precisions': [0.40321071103600675, 0.1511145742198892, 0.07855488607643593, 0.04423542085899547], 'brevity_penalty': 0.8774363173591382, 'length_ratio': 0.8843680786002774, 'translation_length': 230790, 'reference_length': 260966, 'meteor': 0.3229780283998581, 'rouge1': 0.37077469710539124, 'rouge2': 0.15824574075515202, 'rougeL': 0.3018525494847124, 'rougeLsum': 0.3017895259997794}
Generating for the context length: 200
Done
{'bleu': 0.1182973477348703, 'precisions': [0.42795606477988524, 0.1706025720326074, 0.09110392393718583, 0.05252874155797288], 'brevity_penalty': 0.8652573516278745, 'length_ratio': 0.8735697370538691, 'translation_length': 227972, 'reference_length': 260966, 'meteor': 0.34439665664259916, 'rouge1': 0.3934753980725805, 'rouge2': 0.17680558174604313, 'rougeL': 0.32256687583822485, 'rougeLsum': 0.32252509177218214}
Generating for the context length: 300
Done
{'bleu': 0.12209822621014697, 'precisions': [0.4352131300218336, 0.17588316065410062, 0.09497811800178568, 0.05491429486285629], 'brevity_penalty': 0.8637748391098488, 'length_ratio': 0.8722630534245841, 'translation_length': 227631, 'reference_length': 260966, 'meteor': 0.3510299468147969, 'rouge1': 0.4000138503482971, 'rouge2': 0.181585420378426, 'rougeL': 0.32815791792413407, 'rougeLsum': 0.328150388079802}
Generating for the context length: 400
Done
{'bleu': 0.12255825046610065, 'precisions': [0.43718118852477084, 0.17711209596955704, 0.09583102703391118, 0.055524149841559615], 'brevity_penalty': 0.8602369944685146, 'length_ratio': 0.869151536981829, 'translation_length': 226819, 'reference_length': 260966, 'meteor': 0.35142175162812966, 'rouge1': 0.401029466456867, 'rouge2': 0.1819975982535722, 'rougeL': 0.3288385718967255, 'rougeLsum': 0.3288534840206758}
Generating for the context length: 500
Done
{'bleu': 0.12275843785464542, 'precisions': [0.43658814287857795, 0.17754942767950052, 0.0960819889177659, 0.05593054892105755], 'brevity_penalty': 0.85927224361604, 'length_ratio': 0.8683046833687147, 'translation_length': 226598, 'reference_length': 260966, 'meteor': 0.3510548009829209, 'rouge1': 0.40039649305025937, 'rouge2': 0.18245897432539857, 'rougeL': 0.3284684403403602, 'rougeLsum': 0.32841664651662117}
Generating for the context length: 650
Done
{'bleu': 0.12260796544643882, 'precisions': [0.4363032425914229, 0.1773513314950781, 0.09621303227389864, 0.05591217884412473], 'brevity_penalty': 0.8583766250749563, 'length_ratio': 0.8675191404244231, 'translation_length': 226393, 'reference_length': 260966, 'meteor': 0.3504583729851767, 'rouge1': 0.4000570336776107, 'rouge2': 0.1826596272660631, 'rougeL': 0.3290682486923599, 'rougeLsum': 0.3289444772091152}
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.4.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._wlc.bias', 'encoder.layers.4.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.weight', 'encoder.layers.2.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.4.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.4.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.2.context_enforcer._wrc.weight', 'encoder.layers.4.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wlc.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Performing inference using checkpoint-29757
Generating for the context length: 100
Done
{'bleu': 0.10003612705222899, 'precisions': [0.3959851648209145, 0.1438674957274281, 0.0728073454287549, 0.04029593594810986], 'brevity_penalty': 0.8798068893435765, 'length_ratio': 0.8864832966746626, 'translation_length': 231342, 'reference_length': 260966, 'meteor': 0.31465339772486856, 'rouge1': 0.36375326371998973, 'rouge2': 0.15089503428534307, 'rougeL': 0.29542073509061817, 'rougeLsum': 0.29525904326486985}
Generating for the context length: 200
Done
{'bleu': 0.11157175858966797, 'precisions': [0.41994664134987714, 0.16255672277653654, 0.08485274519280055, 0.048166808774513604], 'brevity_penalty': 0.8632787960943377, 'length_ratio': 0.8718262149092219, 'translation_length': 227517, 'reference_length': 260966, 'meteor': 0.3346708329306046, 'rouge1': 0.3845765298133242, 'rouge2': 0.1682445339458192, 'rougeL': 0.3139504659375426, 'rougeLsum': 0.31376581075948773}
Generating for the context length: 300
Done
{'bleu': 0.1144993477113596, 'precisions': [0.4268938826246793, 0.16691102780149214, 0.08791149593034076, 0.05006222130042518], 'brevity_penalty': 0.8604246134041494, 'length_ratio': 0.8693163094042902, 'translation_length': 226862, 'reference_length': 260966, 'meteor': 0.3394676770447302, 'rouge1': 0.3906494562996171, 'rouge2': 0.17226847359027764, 'rougeL': 0.3188470562329002, 'rougeLsum': 0.31882771990421177}
Generating for the context length: 400
Done
{'bleu': 0.1154710337483996, 'precisions': [0.42764165221573586, 0.16837827167374997, 0.0889925034688004, 0.05085995213547426], 'brevity_penalty': 0.8594076187999852, 'length_ratio': 0.8684234727895588, 'translation_length': 226629, 'reference_length': 260966, 'meteor': 0.33994898649209854, 'rouge1': 0.3908549452144654, 'rouge2': 0.17343491208845668, 'rougeL': 0.3192190875536438, 'rougeLsum': 0.31910555083461645}
Generating for the context length: 500
Done
{'bleu': 0.11570863596010993, 'precisions': [0.4272724469252085, 0.16858251760898094, 0.08907954562139357, 0.05083709434216458], 'brevity_penalty': 0.8609872889422542, 'length_ratio': 0.8698106266716737, 'translation_length': 226991, 'reference_length': 260966, 'meteor': 0.3408604129440663, 'rouge1': 0.391304406462213, 'rouge2': 0.1742812826742762, 'rougeL': 0.3199619807090074, 'rougeLsum': 0.3198029046408794}
Generating for the context length: 650
Done
{'bleu': 0.11611016803825859, 'precisions': [0.42799906558121287, 0.16858196664269642, 0.08944669973703669, 0.051364341004889125], 'brevity_penalty': 0.8604987799984398, 'length_ratio': 0.8693814519899143, 'translation_length': 226879, 'reference_length': 260966, 'meteor': 0.3405982015568443, 'rouge1': 0.39167103087490773, 'rouge2': 0.1741245096755088, 'rougeL': 0.31988234799965437, 'rougeLsum': 0.3197451829248713}
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.4.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._wlc.bias', 'encoder.layers.4.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.2.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.4.context_enforcer_layer_norm.weight', 'encoder.layers.2.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.4.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.4.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.2.context_enforcer._wrc.weight', 'encoder.layers.4.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._wlc.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Performing inference using checkpoint-51012
Generating for the context length: 100
Done
{'bleu': 0.10883592037233569, 'precisions': [0.401073099649179, 0.15034341843977905, 0.07789081597020138, 0.0444010794284338], 'brevity_penalty': 0.9056811315293359, 'length_ratio': 0.9098618210801407, 'translation_length': 237443, 'reference_length': 260966, 'meteor': 0.3288416766279796, 'rouge1': 0.3750395394310876, 'rouge2': 0.15948129389897242, 'rougeL': 0.3041237815212952, 'rougeLsum': 0.30403571801459606}
Generating for the context length: 200
Done
{'bleu': 0.12083150988480816, 'precisions': [0.4245894158638366, 0.168537613881308, 0.08979534027335175, 0.05211965223637153], 'brevity_penalty': 0.8932023843092053, 'length_ratio': 0.8985193473479304, 'translation_length': 234483, 'reference_length': 260966, 'meteor': 0.3495072967229206, 'rouge1': 0.3962578876160954, 'rouge2': 0.17727208183543075, 'rougeL': 0.3240548410883889, 'rougeLsum': 0.3239637870001243}
Generating for the context length: 300
Done
{'bleu': 0.12448640123389663, 'precisions': [0.4302988869186233, 0.17360496809623666, 0.09349141540543225, 0.05448600980838162], 'brevity_penalty': 0.8913014937836947, 'length_ratio': 0.8968026486208931, 'translation_length': 234035, 'reference_length': 260966, 'meteor': 0.35499754798002425, 'rouge1': 0.40174954032269605, 'rouge2': 0.18186383032258616, 'rougeL': 0.3282911324757086, 'rougeLsum': 0.3282076682199259}
Generating for the context length: 400
Done
{'bleu': 0.12495038537668919, 'precisions': [0.43175045639357157, 0.17415137150944668, 0.09394927827828302, 0.05481823138916542], 'brevity_penalty': 0.8907238002394383, 'length_ratio': 0.8962815079358997, 'translation_length': 233899, 'reference_length': 260966, 'meteor': 0.3559642498472402, 'rouge1': 0.40305916793050345, 'rouge2': 0.1824112362053269, 'rougeL': 0.32979609841933866, 'rougeLsum': 0.32974801036249046}
Generating for the context length: 500
Done
{'bleu': 0.1254024570368664, 'precisions': [0.4313193503868881, 0.17421075554637583, 0.09450796909816017, 0.05516758729285911], 'brevity_penalty': 0.891352452482985, 'length_ratio': 0.8968486316225102, 'translation_length': 234047, 'reference_length': 260966, 'meteor': 0.3552191653415467, 'rouge1': 0.40285591567650925, 'rouge2': 0.18254238494723324, 'rougeL': 0.3293225721532229, 'rougeLsum': 0.32928602697307985}
Generating for the context length: 650
Done
{'bleu': 0.12459242061960475, 'precisions': [0.4312536947712026, 0.1744408025069337, 0.09408834274327466, 0.05456387011372069], 'brevity_penalty': 0.8887633779698625, 'length_ratio': 0.894514994290444, 'translation_length': 233438, 'reference_length': 260966, 'meteor': 0.35475189518119854, 'rouge1': 0.40201895546392463, 'rouge2': 0.18266026408619807, 'rougeL': 0.32883637875493865, 'rougeLsum': 0.3287266630425866}
