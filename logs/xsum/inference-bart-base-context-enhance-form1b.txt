[rank: 0] Global seed set to 1376
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[rank: 0] Global seed set to 1376
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[rank: 0] Global seed set to 1376
[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 846.08it/s]
['trained-models/bart-base-context-enhance-form2/checkpoint-59514', 'trained-models/bart-base-context-enhance-form2/checkpoint-55263', 'trained-models/bart-base-context-enhance-form2/checkpoint-34008']
Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 1024). Running this sequence through the model will result in indexing errors
Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForContextualRecovery: ['encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.out_proj.bias']
- This IS expected if you are initializing BartForContextualRecovery from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForContextualRecovery from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.1.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.1.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.1.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer_layer_norm.bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Performing inference using checkpoint-59514
Generating for the context length: 100
Done
{'bleu': 0.09904407753248277, 'precisions': [0.39322594871212685, 0.14276001782375614, 0.07309804073602032, 0.04097036698988304], 'brevity_penalty': 0.8698060481126274, 'length_ratio': 0.8775894177785611, 'translation_length': 229021, 'reference_length': 260966, 'meteor': 0.3103181176863558, 'rouge1': 0.35892000207862573, 'rouge2': 0.148987485308348, 'rougeL': 0.2917506865381141, 'rougeLsum': 0.2917287508524135}
Generating for the context length: 200
Done
{'bleu': 0.10904963710374557, 'precisions': [0.4178917929466411, 0.16006887894385619, 0.08345692394865857, 0.04740085898727175], 'brevity_penalty': 0.8550064519782987, 'length_ratio': 0.8645685644873279, 'translation_length': 225623, 'reference_length': 260966, 'meteor': 0.3305234296600141, 'rouge1': 0.3803918832329942, 'rouge2': 0.16502142472877623, 'rougeL': 0.310519240590044, 'rougeLsum': 0.3105148887143824}
Generating for the context length: 300
Done
{'bleu': 0.11292780398672028, 'precisions': [0.4241383447154327, 0.16531369457298328, 0.08747338099637833, 0.05019391915586285], 'brevity_penalty': 0.8525405319075258, 'length_ratio': 0.8624150272449285, 'translation_length': 225061, 'reference_length': 260966, 'meteor': 0.3361008577940777, 'rouge1': 0.3861645089272414, 'rouge2': 0.16982662348011157, 'rougeL': 0.3153020565627547, 'rougeLsum': 0.3152231182405339}
Generating for the context length: 400
Done
{'bleu': 0.11305960990367266, 'precisions': [0.4242397284951139, 0.16536369896059244, 0.08761185834491697, 0.05057148397172189], 'brevity_penalty': 0.8514858916989508, 'length_ratio': 0.8614953672125871, 'translation_length': 224821, 'reference_length': 260966, 'meteor': 0.3357722411845276, 'rouge1': 0.38591411685679744, 'rouge2': 0.16979468193803607, 'rougeL': 0.3145787681287628, 'rougeLsum': 0.31447465283559695}
Generating for the context length: 500
Done
{'bleu': 0.11248316294732369, 'precisions': [0.42447272062332475, 0.16521281642968202, 0.08730879339515453, 0.05036406171963305], 'brevity_penalty': 0.8488275298947708, 'length_ratio': 0.859180889464528, 'translation_length': 224217, 'reference_length': 260966, 'meteor': 0.33488990328750895, 'rouge1': 0.3859631893773534, 'rouge2': 0.1695534417881637, 'rougeL': 0.31506653954641534, 'rougeLsum': 0.3150249633323986}
Generating for the context length: 650
Done
{'bleu': 0.11224031781329631, 'precisions': [0.42375899184263605, 0.16484456518673798, 0.08734808165859083, 0.05023409963701404], 'brevity_penalty': 0.8482766238504805, 'length_ratio': 0.8587018998643502, 'translation_length': 224092, 'reference_length': 260966, 'meteor': 0.3342001284870782, 'rouge1': 0.38496043526933976, 'rouge2': 0.16874039631716636, 'rougeL': 0.31400362290001693, 'rougeLsum': 0.3139417691173495}
Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForContextualRecovery: ['encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.out_proj.bias']
- This IS expected if you are initializing BartForContextualRecovery from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForContextualRecovery from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.1.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.1.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.1.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer_layer_norm.bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForContextualRecovery: ['encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.out_proj.bias']
- This IS expected if you are initializing BartForContextualRecovery from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForContextualRecovery from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.1.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.1.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.1.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.1.context_enforcer._wlc.bias', 'encoder.layers.1.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.1.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.1.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.1.context_enforcer_layer_norm.bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Performing inference using checkpoint-34008
Generating for the context length: 100
Done
{'bleu': 0.09223601641459606, 'precisions': [0.3846999630561371, 0.13515677797424527, 0.06756585118024074, 0.0372032890313906], 'brevity_penalty': 0.862647557793925, 'length_ratio': 0.8712705869730156, 'translation_length': 227372, 'reference_length': 260966, 'meteor': 0.2996240965010343, 'rouge1': 0.34922036982463267, 'rouge2': 0.14106373978891082, 'rougeL': 0.28283654947171555, 'rougeLsum': 0.28279800800123867}
Generating for the context length: 200
Done
{'bleu': 0.10213693353165793, 'precisions': [0.40854846403145495, 0.15134257020381525, 0.07774245857983764, 0.0435164119189361], 'brevity_penalty': 0.8492856880640073, 'length_ratio': 0.8595794088118759, 'translation_length': 224321, 'reference_length': 260966, 'meteor': 0.31886717072433546, 'rouge1': 0.37016197578798016, 'rouge2': 0.1564057710011552, 'rougeL': 0.3003088776385878, 'rougeLsum': 0.30027565096688724}
Generating for the context length: 300
Done
{'bleu': 0.10643710537987784, 'precisions': [0.41684448349441444, 0.15806500363650103, 0.08247674743523213, 0.04659022393348636], 'brevity_penalty': 0.8437893279520152, 'length_ratio': 0.854808672394105, 'translation_length': 223076, 'reference_length': 260966, 'meteor': 0.3254662243675653, 'rouge1': 0.37740112129550896, 'rouge2': 0.16208946225412657, 'rougeL': 0.3068446639007052, 'rougeLsum': 0.3068277106356362}
Generating for the context length: 400
Done
{'bleu': 0.10643980446560215, 'precisions': [0.4180150080159511, 0.15867743568633574, 0.0826346287491313, 0.04658176055628872], 'brevity_penalty': 0.8420401720863819, 'length_ratio': 0.8532950652575432, 'translation_length': 222681, 'reference_length': 260966, 'meteor': 0.3262102751390317, 'rouge1': 0.3782539257571227, 'rouge2': 0.16276653992820242, 'rougeL': 0.30782867045232054, 'rougeLsum': 0.3077822752606605}
Generating for the context length: 500
Done
{'bleu': 0.10593598394205614, 'precisions': [0.4175387021953719, 0.15816495685204657, 0.08227227227227227, 0.04631604639563635], 'brevity_penalty': 0.8410958894527698, 'length_ratio': 0.8524788669788401, 'translation_length': 222468, 'reference_length': 260966, 'meteor': 0.3254082942173555, 'rouge1': 0.37753377909127145, 'rouge2': 0.16214492723529872, 'rougeL': 0.30670452355123257, 'rougeLsum': 0.30665875427775535}
Generating for the context length: 650
Done
{'bleu': 0.10505136986975361, 'precisions': [0.4165723739671005, 0.15734965058220463, 0.08171360571201904, 0.04605350725409291], 'brevity_penalty': 0.8382496616367411, 'length_ratio': 0.8500226083091283, 'translation_length': 221827, 'reference_length': 260966, 'meteor': 0.32371927767313186, 'rouge1': 0.37620910454887885, 'rouge2': 0.161296137143268, 'rougeL': 0.305448360283046, 'rougeLsum': 0.3054286983102217}
