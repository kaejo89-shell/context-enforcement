[rank: 0] Global seed set to 1376
[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 768.14it/s]
[2023-09-06 06:05:16,168] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Some weights of BartForContextualRecovery were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.layers.5.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._wlc.weight', 'encoder.layers.3.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.2.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.2.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._wrc.bias', 'encoder.layers.4.context_enforcer_layer_norm.weight', 'encoder.layers.4.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.5.context_enforcer._wrc.weight', 'encoder.layers.4.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.2.context_enforcer._wlc.bias', 'encoder.layers.2.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._wrc.bias', 'encoder.layers.4.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.5.context_enforcer._wlc.bias', 'encoder.layers.5.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.in_proj_weight', 'encoder.layers.4.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.2.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.5.context_enforcer_layer_norm.bias', 'encoder.layers.3.context_enforcer._wrc.bias', 'encoder.layers.2.context_enforcer_layer_norm.bias', 'encoder.layers.2.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer_layer_norm.weight', 'encoder.layers.4.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._left_mha.in_proj_weight', 'encoder.layers.3.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.2.context_enforcer_layer_norm.weight', 'encoder.layers.3.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer_layer_norm.weight', 'encoder.layers.2.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.4.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._wrc.bias', 'encoder.layers.5.context_enforcer._wlc.weight', 'encoder.layers.2.context_enforcer._right_mha.out_proj.bias', 'encoder.layers.3.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.2.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.4.context_enforcer._wlc.weight', 'encoder.layers.4.context_enforcer._wlc.bias', 'encoder.layers.3.context_enforcer_layer_norm.bias', 'encoder.layers.5.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._wrc.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.weight', 'encoder.layers.3.context_enforcer._wlc.weight', 'encoder.layers.5.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.4.context_enforcer_layer_norm.bias', 'encoder.layers.4.context_enforcer._left_mha.in_proj_bias', 'encoder.layers.4.context_enforcer._left_mha.out_proj.bias', 'encoder.layers.5.context_enforcer._right_mha.in_proj_bias', 'encoder.layers.3.context_enforcer._right_mha.out_proj.weight', 'encoder.layers.2.context_enforcer._right_mha.in_proj_weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
{'loss': 4.131, 'learning_rate': 1.3280759924724438e-06, 'epoch': 0.12}
{'loss': 2.9838, 'learning_rate': 2.6722824625862535e-06, 'epoch': 0.24}
{'loss': 2.8003, 'learning_rate': 4.016488932700062e-06, 'epoch': 0.35}
{'loss': 2.685, 'learning_rate': 5.360695402813872e-06, 'epoch': 0.47}
{'loss': 2.6108, 'learning_rate': 6.7049018729276815e-06, 'epoch': 0.59}
{'loss': 2.547, 'learning_rate': 8.049108343041492e-06, 'epoch': 0.71}
{'loss': 2.5021, 'learning_rate': 9.3933148131553e-06, 'epoch': 0.82}
{'loss': 2.4613, 'learning_rate': 1.0734832870328883e-05, 'epoch': 0.94}
{'eval_loss': 2.130145311355591, 'eval_runtime': 68.0045, 'eval_samples_per_second': 166.665, 'eval_steps_per_second': 20.837, 'epoch': 1.0}
{'loss': 2.4131, 'learning_rate': 1.2079039340442693e-05, 'epoch': 1.06}
{'loss': 2.3734, 'learning_rate': 1.3423245810556502e-05, 'epoch': 1.18}
{'loss': 2.3483, 'learning_rate': 1.4767452280670312e-05, 'epoch': 1.29}
{'loss': 2.3254, 'learning_rate': 1.611165875078412e-05, 'epoch': 1.41}
{'loss': 2.3013, 'learning_rate': 1.745586522089793e-05, 'epoch': 1.53}
{'loss': 2.2765, 'learning_rate': 1.880007169101174e-05, 'epoch': 1.65}
{'loss': 2.2516, 'learning_rate': 2.014427816112555e-05, 'epoch': 1.76}
{'loss': 2.2448, 'learning_rate': 2.1488484631239356e-05, 'epoch': 1.88}
{'loss': 2.2325, 'learning_rate': 2.283269110135317e-05, 'epoch': 2.0}
{'eval_loss': 1.9961938858032227, 'eval_runtime': 68.1851, 'eval_samples_per_second': 166.224, 'eval_steps_per_second': 20.782, 'epoch': 2.0}
{'loss': 2.155, 'learning_rate': 2.4176897571466977e-05, 'epoch': 2.12}
{'loss': 2.1459, 'learning_rate': 2.551841562864056e-05, 'epoch': 2.23}
{'loss': 2.1491, 'learning_rate': 2.686262209875437e-05, 'epoch': 2.35}
{'loss': 2.136, 'learning_rate': 2.8206828568868177e-05, 'epoch': 2.47}
{'loss': 2.1158, 'learning_rate': 2.955103503898199e-05, 'epoch': 2.59}
{'loss': 2.1145, 'learning_rate': 3.08952415090958e-05, 'epoch': 2.71}
{'loss': 2.1102, 'learning_rate': 3.223944797920961e-05, 'epoch': 2.82}
{'loss': 2.0952, 'learning_rate': 3.358365444932342e-05, 'epoch': 2.94}
{'eval_loss': 1.9359230995178223, 'eval_runtime': 68.0761, 'eval_samples_per_second': 166.49, 'eval_steps_per_second': 20.815, 'epoch': 3.0}
{'loss': 2.0443, 'learning_rate': 3.492786091943723e-05, 'epoch': 3.06}
{'loss': 1.9964, 'learning_rate': 3.627206738955103e-05, 'epoch': 3.18}
{'loss': 2.0, 'learning_rate': 3.7616273859664845e-05, 'epoch': 3.29}
{'loss': 2.0001, 'learning_rate': 3.896048032977866e-05, 'epoch': 3.41}
{'loss': 2.0024, 'learning_rate': 4.030468679989246e-05, 'epoch': 3.53}
{'loss': 2.0051, 'learning_rate': 4.1648893270006275e-05, 'epoch': 3.65}
{'loss': 1.9872, 'learning_rate': 4.299041132717985e-05, 'epoch': 3.76}
{'loss': 2.0015, 'learning_rate': 4.433461779729366e-05, 'epoch': 3.88}
{'loss': 1.9798, 'learning_rate': 4.5678824267407474e-05, 'epoch': 4.0}
{'eval_loss': 1.899707317352295, 'eval_runtime': 68.2775, 'eval_samples_per_second': 165.999, 'eval_steps_per_second': 20.754, 'epoch': 4.0}
{'loss': 1.8737, 'learning_rate': 4.7023030737521286e-05, 'epoch': 4.12}
{'loss': 1.8847, 'learning_rate': 4.836454879469487e-05, 'epoch': 4.23}
{'loss': 1.9038, 'learning_rate': 4.970606685186845e-05, 'epoch': 4.35}
{'loss': 1.8934, 'learning_rate': 5.1050273321982256e-05, 'epoch': 4.47}
{'loss': 1.8966, 'learning_rate': 5.239447979209607e-05, 'epoch': 4.59}
{'loss': 1.897, 'learning_rate': 5.373868626220988e-05, 'epoch': 4.7}
{'loss': 1.8959, 'learning_rate': 5.5082892732323685e-05, 'epoch': 4.82}
{'loss': 1.8996, 'learning_rate': 5.642709920243749e-05, 'epoch': 4.94}
{'eval_loss': 1.885897159576416, 'eval_runtime': 68.0845, 'eval_samples_per_second': 166.47, 'eval_steps_per_second': 20.812, 'epoch': 5.0}
{'loss': 1.8326, 'learning_rate': 5.776861725961108e-05, 'epoch': 5.06}
{'loss': 1.7707, 'learning_rate': 5.911282372972489e-05, 'epoch': 5.18}
{'loss': 1.7858, 'learning_rate': 5.975390257437209e-05, 'epoch': 5.29}
{'loss': 1.7953, 'learning_rate': 5.903008661664294e-05, 'epoch': 5.41}
{'loss': 1.7989, 'learning_rate': 5.830627065891379e-05, 'epoch': 5.53}
{'loss': 1.7991, 'learning_rate': 5.758245470118465e-05, 'epoch': 5.65}
{'loss': 1.8014, 'learning_rate': 5.68586387434555e-05, 'epoch': 5.76}
{'loss': 1.7979, 'learning_rate': 5.613482278572635e-05, 'epoch': 5.88}
{'loss': 1.8009, 'learning_rate': 5.54110068279972e-05, 'epoch': 6.0}
{'eval_loss': 1.8607298135757446, 'eval_runtime': 67.9984, 'eval_samples_per_second': 166.68, 'eval_steps_per_second': 20.839, 'epoch': 6.0}
{'loss': 1.6531, 'learning_rate': 5.468719087026806e-05, 'epoch': 6.12}
{'loss': 1.6557, 'learning_rate': 5.3963374912538906e-05, 'epoch': 6.23}
{'loss': 1.6737, 'learning_rate': 5.3239558954809754e-05, 'epoch': 6.35}
{'loss': 1.6745, 'learning_rate': 5.2515742997080617e-05, 'epoch': 6.47}
{'loss': 1.6726, 'learning_rate': 5.179482230318238e-05, 'epoch': 6.59}
{'loss': 1.6864, 'learning_rate': 5.107100634545323e-05, 'epoch': 6.7}
{'loss': 1.6765, 'learning_rate': 5.0347190387724084e-05, 'epoch': 6.82}
{'loss': 1.6794, 'learning_rate': 4.962337442999493e-05, 'epoch': 6.94}
{'eval_loss': 1.8548866510391235, 'eval_runtime': 68.2354, 'eval_samples_per_second': 166.101, 'eval_steps_per_second': 20.766, 'epoch': 7.0}
{'loss': 1.6071, 'learning_rate': 4.889955847226578e-05, 'epoch': 7.06}
{'loss': 1.5414, 'learning_rate': 4.8175742514536644e-05, 'epoch': 7.17}
{'loss': 1.5563, 'learning_rate': 4.745192655680749e-05, 'epoch': 7.29}
{'loss': 1.5618, 'learning_rate': 4.672811059907834e-05, 'epoch': 7.41}
{'loss': 1.5707, 'learning_rate': 4.60042946413492e-05, 'epoch': 7.53}
{'loss': 1.5749, 'learning_rate': 4.5280478683620045e-05, 'epoch': 7.65}
{'loss': 1.5752, 'learning_rate': 4.4556662725890894e-05, 'epoch': 7.76}
{'loss': 1.5756, 'learning_rate': 4.383284676816175e-05, 'epoch': 7.88}
{'loss': 1.5859, 'learning_rate': 4.31090308104326e-05, 'epoch': 8.0}
{'eval_loss': 1.8585319519042969, 'eval_runtime': 68.0171, 'eval_samples_per_second': 166.635, 'eval_steps_per_second': 20.833, 'epoch': 8.0}
{'loss': 1.4436, 'learning_rate': 4.2385214852703454e-05, 'epoch': 8.12}
{'loss': 1.4595, 'learning_rate': 4.166139889497431e-05, 'epoch': 8.23}
{'loss': 1.4652, 'learning_rate': 4.0939030569160615e-05, 'epoch': 8.35}
{'loss': 1.4701, 'learning_rate': 4.0218109875262385e-05, 'epoch': 8.47}
{'loss': 1.4722, 'learning_rate': 3.9494293917533234e-05, 'epoch': 8.59}
{'loss': 1.4832, 'learning_rate': 3.877192559171955e-05, 'epoch': 8.7}
{'loss': 1.4902, 'learning_rate': 3.80481096339904e-05, 'epoch': 8.82}
{'loss': 1.4907, 'learning_rate': 3.732429367626125e-05, 'epoch': 8.94}
{'eval_loss': 1.870273470878601, 'eval_runtime': 68.5238, 'eval_samples_per_second': 165.402, 'eval_steps_per_second': 20.679, 'epoch': 9.0}
{'loss': 1.4229, 'learning_rate': 3.6601925350447564e-05, 'epoch': 9.06}
{'loss': 1.3728, 'learning_rate': 3.587810939271841e-05, 'epoch': 9.17}
{'loss': 1.3722, 'learning_rate': 3.515429343498927e-05, 'epoch': 9.29}
{'loss': 1.3903, 'learning_rate': 3.443047747726012e-05, 'epoch': 9.41}
{'loss': 1.3892, 'learning_rate': 3.3706661519530965e-05, 'epoch': 9.53}
{'loss': 1.3934, 'learning_rate': 3.298284556180182e-05, 'epoch': 9.64}
{'loss': 1.4008, 'learning_rate': 3.225902960407267e-05, 'epoch': 9.76}
{'loss': 1.4002, 'learning_rate': 3.153521364634352e-05, 'epoch': 9.88}
{'loss': 1.4064, 'learning_rate': 3.081139768861438e-05, 'epoch': 10.0}
{'eval_loss': 1.8847192525863647, 'eval_runtime': 90.1549, 'eval_samples_per_second': 125.717, 'eval_steps_per_second': 15.717, 'epoch': 10.0}
{'loss': 1.2926, 'learning_rate': 3.0087581730885226e-05, 'epoch': 10.12}
{'loss': 1.297, 'learning_rate': 2.936376577315608e-05, 'epoch': 10.23}
{'loss': 1.3107, 'learning_rate': 2.863994981542693e-05, 'epoch': 10.35}
{'loss': 1.3098, 'learning_rate': 2.7916133857697782e-05, 'epoch': 10.47}
{'loss': 1.3196, 'learning_rate': 2.7193765531884095e-05, 'epoch': 10.59}
{'loss': 1.3178, 'learning_rate': 2.6469949574154947e-05, 'epoch': 10.7}
{'loss': 1.3261, 'learning_rate': 2.5746133616425796e-05, 'epoch': 10.82}
{'loss': 1.327, 'learning_rate': 2.502231765869665e-05, 'epoch': 10.94}
{'eval_loss': 1.8934186697006226, 'eval_runtime': 69.3882, 'eval_samples_per_second': 163.342, 'eval_steps_per_second': 20.421, 'epoch': 11.0}
{'loss': 1.2786, 'learning_rate': 2.4298501700967503e-05, 'epoch': 11.06}
{'loss': 1.2258, 'learning_rate': 2.357613337515381e-05, 'epoch': 11.17}
{'loss': 1.2365, 'learning_rate': 2.285231741742466e-05, 'epoch': 11.29}
{'loss': 1.2474, 'learning_rate': 2.2128501459695517e-05, 'epoch': 11.41}
{'loss': 1.2461, 'learning_rate': 2.1404685501966366e-05, 'epoch': 11.53}
{'loss': 1.2554, 'learning_rate': 2.0680869544237218e-05, 'epoch': 11.64}
{'loss': 1.2562, 'learning_rate': 1.995850121842353e-05, 'epoch': 11.76}
{'loss': 1.2611, 'learning_rate': 1.9234685260694383e-05, 'epoch': 11.88}
{'loss': 1.2605, 'learning_rate': 1.851086930296523e-05, 'epoch': 12.0}
{'eval_loss': 1.92034113407135, 'eval_runtime': 68.0258, 'eval_samples_per_second': 166.613, 'eval_steps_per_second': 20.83, 'epoch': 12.0}
{'train_runtime': 44209.9571, 'train_samples_per_second': 69.23, 'train_steps_per_second': 1.442, 'train_loss': 1.7985541406803613, 'epoch': 12.0}
