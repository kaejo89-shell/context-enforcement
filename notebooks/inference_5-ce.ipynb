{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 1376\n",
      "Global seed set to 1376\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Global seed set to 1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from functools import partial\n",
    "import nltk\n",
    "import pickle as pk\n",
    "import torch\n",
    "from context_enforcement.models.context_enforcer import compute_context_boundary\n",
    "from context_enforcement.trainers.train_t5 import model_init\n",
    "from context_enforcement.data.common import create_text_tokenizer, SmartCollator\n",
    "from context_enforcement.trainers.common import get_dataset_specified_tasks\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import sys\n",
    "\n",
    "seed_everything(1376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs[\"model_base\"] = \"t5-base\"\n",
    "configs[\"task_type\"] = \"xsum\"\n",
    "configs[\"context_max_len\"] = 512\n",
    "configs[\"task_type\"] = \"xsum\"\n",
    "from argparse import Namespace\n",
    "configs = Namespace(**configs)\n",
    "#configs= pk.load(open(\"trained_models/t5models/t5-base_model_ce/train_args.ap\",'rb'))\n",
    "context_max_len=configs.context_max_len\n",
    "context_sampling_bounds=(0.1, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "namedtuple() missing 1 required positional argument: 'field_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m namedtuple\n\u001b[0;32m----> 2\u001b[0m elems \u001b[39m=\u001b[39m namedtuple(configs)\n",
      "\u001b[0;31mTypeError\u001b[0m: namedtuple() missing 1 required positional argument: 'field_names'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "100%|██████████| 3/3 [00:00<00:00, 455.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_text_tokenizer(configs.model_base)\n",
    "\n",
    "task_dataset_gen = get_dataset_specified_tasks(configs.task_type)\n",
    "\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "test_dataset = None\n",
    "if task_dataset_gen is not None:\n",
    "    raw_dataset = task_dataset_gen(tokenizer=tokenizer, )\n",
    "    train_dataset = raw_dataset['train']\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    test_dataset = raw_dataset['test']\n",
    "\n",
    "model_builder = model_init(\n",
    "        vocab_size=len(train_dataset.tokenizer),\n",
    "        model_base=configs.model_base,\n",
    "        is_baseline=False,\n",
    "        context_max_len = context_max_len,\n",
    "        context_sampling_bounds = context_sampling_bounds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ContextEnforcementSeqModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.1.context_enforcer_layer_norm.bias', 'encoder.block.1.context_enforcer._left_mha.out_proj.weight', 'encoder.block.0.context_enforcer._right_mha.in_proj_weight', 'encoder.block.8.context_enforcer._left_mha.in_proj_weight', 'encoder.block.8.context_enforcer._right_mha.out_proj.bias', 'encoder.block.9.context_enforcer_layer_norm.weight', 'encoder.block.1.context_enforcer._right_mha.out_proj.weight', 'encoder.block.4.context_enforcer._wlc.bias', 'encoder.block.10.context_enforcer._right_mha.out_proj.bias', 'encoder.block.7.context_enforcer._right_mha.in_proj_weight', 'encoder.block.7.context_enforcer._left_mha.in_proj_weight', 'encoder.block.1.context_enforcer._wlc.bias', 'encoder.block.2.context_enforcer._left_mha.in_proj_weight', 'encoder.block.3.context_enforcer._wrc.bias', 'encoder.block.11.context_enforcer._right_mha.in_proj_bias', 'encoder.block.0.context_enforcer._right_mha.out_proj.bias', 'encoder.block.10.context_enforcer._left_mha.in_proj_bias', 'encoder.block.1.context_enforcer_layer_norm.weight', 'encoder.block.0.context_enforcer_layer_norm.weight', 'encoder.block.5.context_enforcer._right_mha.out_proj.weight', 'encoder.block.7.context_enforcer_layer_norm.bias', 'encoder.block.0.context_enforcer._wrc.weight', 'encoder.block.1.context_enforcer._right_mha.in_proj_bias', 'encoder.block.3.context_enforcer._wlc.weight', 'encoder.block.2.context_enforcer._left_mha.in_proj_bias', 'encoder.block.1.context_enforcer._right_mha.in_proj_weight', 'encoder.block.9.context_enforcer._wrc.weight', 'encoder.block.5.context_enforcer._wrc.weight', 'encoder.block.3.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer._wrc.weight', 'encoder.block.9.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer_layer_norm.bias', 'encoder.block.8.context_enforcer._right_mha.out_proj.weight', 'encoder.block.10.context_enforcer_layer_norm.weight', 'encoder.block.8.context_enforcer._right_mha.in_proj_bias', 'encoder.block.0.context_enforcer._left_mha.out_proj.bias', 'encoder.block.5.context_enforcer_layer_norm.weight', 'encoder.block.5.context_enforcer._wlc.bias', 'encoder.block.3.context_enforcer_layer_norm.weight', 'encoder.block.3.context_enforcer_layer_norm.bias', 'encoder.block.4.context_enforcer._left_mha.out_proj.weight', 'encoder.block.8.context_enforcer._left_mha.out_proj.weight', 'encoder.block.3.context_enforcer._left_mha.out_proj.weight', 'encoder.block.9.context_enforcer._wlc.weight', 'encoder.block.11.context_enforcer._right_mha.out_proj.weight', 'encoder.block.6.context_enforcer._left_mha.in_proj_bias', 'encoder.block.4.context_enforcer._wlc.weight', 'encoder.block.5.context_enforcer._wrc.bias', 'encoder.block.11.context_enforcer._left_mha.out_proj.weight', 'encoder.block.7.context_enforcer._left_mha.out_proj.bias', 'encoder.block.3.context_enforcer._left_mha.in_proj_bias', 'encoder.block.11.context_enforcer_layer_norm.weight', 'encoder.block.10.context_enforcer._right_mha.out_proj.weight', 'encoder.block.8.context_enforcer._wlc.weight', 'encoder.block.6.context_enforcer_layer_norm.bias', 'encoder.block.4.context_enforcer_layer_norm.bias', 'encoder.block.0.context_enforcer._wlc.bias', 'encoder.block.10.context_enforcer_layer_norm.bias', 'encoder.block.0.context_enforcer._wrc.bias', 'encoder.block.0.context_enforcer._right_mha.in_proj_bias', 'encoder.block.4.context_enforcer._left_mha.in_proj_weight', 'encoder.block.6.context_enforcer._wrc.weight', 'encoder.block.2.context_enforcer._left_mha.out_proj.weight', 'encoder.block.7.context_enforcer_layer_norm.weight', 'encoder.block.3.context_enforcer._left_mha.out_proj.bias', 'encoder.block.9.context_enforcer._left_mha.in_proj_bias', 'encoder.block.5.context_enforcer_layer_norm.bias', 'encoder.block.10.context_enforcer._left_mha.in_proj_weight', 'encoder.block.6.context_enforcer._left_mha.out_proj.weight', 'encoder.block.11.context_enforcer._wrc.weight', 'encoder.block.8.context_enforcer._wlc.bias', 'encoder.block.2.context_enforcer_layer_norm.weight', 'encoder.block.11.context_enforcer._left_mha.in_proj_weight', 'encoder.block.5.context_enforcer._left_mha.out_proj.weight', 'encoder.block.6.context_enforcer._left_mha.in_proj_weight', 'encoder.block.11.context_enforcer._right_mha.in_proj_weight', 'encoder.block.0.context_enforcer._left_mha.in_proj_weight', 'encoder.block.4.context_enforcer._right_mha.out_proj.weight', 'encoder.block.10.context_enforcer._right_mha.in_proj_weight', 'encoder.block.2.context_enforcer._wlc.bias', 'encoder.block.4.context_enforcer._wrc.bias', 'encoder.block.6.context_enforcer._right_mha.out_proj.weight', 'encoder.block.0.context_enforcer._wlc.weight', 'encoder.block.0.context_enforcer._left_mha.in_proj_bias', 'encoder.block.7.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._right_mha.in_proj_bias', 'encoder.block.8.context_enforcer_layer_norm.weight', 'encoder.block.3.context_enforcer._right_mha.in_proj_bias', 'encoder.block.9.context_enforcer._wlc.bias', 'encoder.block.6.context_enforcer._wlc.weight', 'encoder.block.2.context_enforcer._right_mha.in_proj_bias', 'encoder.block.8.context_enforcer_layer_norm.bias', 'encoder.block.5.context_enforcer._right_mha.in_proj_weight', 'encoder.block.9.context_enforcer_layer_norm.bias', 'encoder.block.4.context_enforcer._wrc.weight', 'encoder.block.10.context_enforcer._left_mha.out_proj.bias', 'encoder.block.7.context_enforcer._wlc.bias', 'encoder.block.11.context_enforcer._wlc.bias', 'encoder.block.11.context_enforcer._right_mha.out_proj.bias', 'encoder.block.11.context_enforcer_layer_norm.bias', 'encoder.block.5.context_enforcer._left_mha.out_proj.bias', 'encoder.block.9.context_enforcer._left_mha.out_proj.weight', 'encoder.block.0.context_enforcer_layer_norm.bias', 'encoder.block.0.context_enforcer._right_mha.out_proj.weight', 'encoder.block.1.context_enforcer._right_mha.out_proj.bias', 'encoder.block.11.context_enforcer._wrc.bias', 'encoder.block.0.context_enforcer._left_mha.out_proj.weight', 'encoder.block.7.context_enforcer._wlc.weight', 'encoder.block.6.context_enforcer._right_mha.in_proj_bias', 'encoder.block.7.context_enforcer._left_mha.out_proj.weight', 'encoder.block.6.context_enforcer_layer_norm.weight', 'encoder.block.2.context_enforcer._wlc.weight', 'encoder.block.4.context_enforcer_layer_norm.weight', 'encoder.block.6.context_enforcer._left_mha.out_proj.bias', 'encoder.block.4.context_enforcer._right_mha.out_proj.bias', 'encoder.block.1.context_enforcer._wrc.weight', 'encoder.block.4.context_enforcer._right_mha.in_proj_bias', 'encoder.block.10.context_enforcer._wrc.weight', 'encoder.block.6.context_enforcer._wrc.bias', 'encoder.block.9.context_enforcer._left_mha.out_proj.bias', 'encoder.block.5.context_enforcer._right_mha.in_proj_bias', 'encoder.block.5.context_enforcer._right_mha.out_proj.bias', 'encoder.block.7.context_enforcer._left_mha.in_proj_bias', 'encoder.block.5.context_enforcer._left_mha.in_proj_bias', 'encoder.block.1.context_enforcer._wrc.bias', 'encoder.block.7.context_enforcer._right_mha.out_proj.bias', 'encoder.block.6.context_enforcer._right_mha.out_proj.bias', 'encoder.block.2.context_enforcer._right_mha.out_proj.bias', 'encoder.block.2.context_enforcer._wrc.bias', 'encoder.block.8.context_enforcer._right_mha.in_proj_weight', 'encoder.block.3.context_enforcer._right_mha.out_proj.bias', 'encoder.block.7.context_enforcer._wrc.bias', 'encoder.block.8.context_enforcer._left_mha.in_proj_bias', 'encoder.block.3.context_enforcer._wlc.bias', 'encoder.block.9.context_enforcer._left_mha.in_proj_weight', 'encoder.block.11.context_enforcer._left_mha.in_proj_bias', 'encoder.block.6.context_enforcer._wlc.bias', 'encoder.block.6.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._wlc.bias', 'encoder.block.9.context_enforcer._wrc.bias', 'encoder.block.2.context_enforcer._left_mha.out_proj.bias', 'encoder.block.9.context_enforcer._right_mha.out_proj.bias', 'encoder.block.9.context_enforcer._right_mha.in_proj_bias', 'encoder.block.1.context_enforcer._left_mha.in_proj_weight', 'encoder.block.8.context_enforcer._left_mha.out_proj.bias', 'encoder.block.5.context_enforcer._wlc.weight', 'encoder.block.5.context_enforcer._left_mha.in_proj_weight', 'encoder.block.4.context_enforcer._left_mha.out_proj.bias', 'encoder.block.3.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._left_mha.out_proj.weight', 'encoder.block.10.context_enforcer._wlc.weight', 'encoder.block.7.context_enforcer._right_mha.in_proj_bias', 'encoder.block.9.context_enforcer._right_mha.in_proj_weight', 'encoder.block.4.context_enforcer._right_mha.in_proj_weight', 'encoder.block.1.context_enforcer._wlc.weight', 'encoder.block.4.context_enforcer._left_mha.in_proj_bias', 'encoder.block.3.context_enforcer._wrc.weight', 'encoder.block.1.context_enforcer._left_mha.in_proj_bias', 'encoder.block.10.context_enforcer._wrc.bias', 'encoder.block.2.context_enforcer._right_mha.out_proj.weight', 'encoder.block.8.context_enforcer._wrc.bias', 'encoder.block.7.context_enforcer._wrc.weight', 'encoder.block.3.context_enforcer._left_mha.in_proj_weight', 'encoder.block.11.context_enforcer._left_mha.out_proj.bias', 'encoder.block.11.context_enforcer._wlc.weight', 'encoder.block.1.context_enforcer._left_mha.out_proj.bias', 'encoder.block.8.context_enforcer._wrc.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = model_builder()\n",
    "train_model_path = \"trained_models/t5models/t5-base_model_ce2/checkpoint-17004/pytorch_model.bin\"\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 332)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te1= test_dataset[80]\n",
    "b_input_ids = te1.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = te1.attention_mask.view(1, -1).to(device)\n",
    "context_boundary = compute_context_boundary(512,context_max_len=200)\n",
    "context_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[226, 155, 192]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids=torch.randint(1,300,(1,512),device=device)\n",
    "attention_mask=torch.ones_like(torch.randint(1,300,(1,512)),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A man has been arrested on suspicion of murder after a man was stabbed to death in a street attack.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    bb=generator.generate(input_ids=b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                #context_boundary =context_boundary,\n",
    "                eos_token_id=test_dataset.tokenizer.eos_token_id,\n",
    "        max_length=150,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        num_beams=10,\n",
    "        )\n",
    "test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 34-year-old woman who was injured in an attack by a knifeman on a Swiss train has died in hospital, police say.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.tokenizer.decode(te1.labels,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1258])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te1.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media playback is not supported on this device\n",
      "The Olympic silver medallist accused the organisation of \"ageism\" and having \"zero regard\" for her welfare.\n",
      "She is the latest high-profile cyclist to come forward after Jess Varnish, Nicole Cooke and Emma Pooley criticised the World Class programme.\n",
      "Houvenaghel told the BBC she felt \"vindicated\" by a leaked draft report detailing British Cycling's failures.\n",
      "The report said British Cycling \"sanitised\" its own investigation into claims former technical director Shane Sutton used sexist language towards Varnish, who went public last April about her treatment.\n",
      "British Cycling subsequently admitted it did not pay \"sufficient care and attention\" to the wellbeing of staff and athletes at the expense of winning medals, an approach Houvenaghel attested to in her BBC interview.\n",
      "Both Sutton and predecessor Sir Dave Brailsford have now left British Cycling.\n",
      "Houvenaghel, 42, spoke to BBC Sport during its State of Sport week, which on Thursday examines the issue of athlete welfare versus a win-at-all-costs culture.\n",
      "A government-commissioned review, headed by 11-time Paralympic champion Baroness Grey-Thompson, into safety and wellbeing in British sport, is due to be published imminently.\n",
      "It is expected to recommend significant reforms designed to improve the way athletes are treated by governing bodies.\n",
      "Houvenaghel claimed:\n",
      "British Cycling said it \"has acknowledged and takes very seriously previous cultural and governance failings in the World Class Programme\".\n",
      "It said it has accepted the draft report's findings and already put into a place a 39-point action plan to \"systematically address the cultural and behavioural shortcomings\".\n",
      "The statement added: \"Our new chair Jonathan Browning has apologised for instances where we have fallen short in our commitment to athlete welfare and has offered to meet with anyone who can help improve British Cycling.\"\n",
      "Who else has spoken out?\n",
      "Houvenaghel won silver in the individual pursuit at the Beijing Olympics in 2008, and gold in the World Championship team pursuit in 2008, 2009 and 2011.\n",
      "She retired in 2014, aged 39, after withdrawing from the Commonwealth Games in Glasgow with a back injury.\n",
      "Houvenaghel was critical of both Sutton and her team-mates in the aftermath of the London 2012 Olympics, where she was left out of all three team pursuit races as Dani King, Laura Trott and Joanna Rowsell-Shand won gold in a world record time.\n",
      "Speaking to BBC Sport this week, the Northern Irish rider said that experience was \"very traumatic\" and she felt \"torment\" at having \"no explanation\" for her last-minute omission.\n",
      "At the time, Brailsford, then performance director, defended the selection saying they had to \"take the personal element out of it, and look at the data and be professional\".\n",
      "He added: \"I think when a team steps up and makes six world records on the trot and a gold medal, then I don't think you can argue with that.\"\n",
      "British Cycling reiterated that point on Thursday, adding it was \"proud to support Wendy in what was a wonderfully successful cycling career\" and she was \"part of a pioneering generation of riders who set new standards of excellence\", but was dropped in London 2012 \"based on her performance\".\n",
      "Other elite cyclists, including King and Roswell-Shand have praised the leadership at British Cycling.\n",
      "Asked whether she was simply not good enough for the 2012 team, Houvenaghel replied: \"It was definitely not about performance. I don't think the fastest team on the day were permitted to race.\n",
      "\"There are certain chosen riders on the team who will not have experienced the culture of fear and will not have been on the receiving end of that - the bullying, the harassment, being frozen out of opportunities.\n",
      "\"It was horrid - it was not the training environment I expected. There was no choice. If you rocked the boat, you were out. There was no alternative.\n",
      "\"Medals at any cost, that's how it was whenever I was there, certainly in 2012.\"\n",
      "Houvenaghel said she also witnessed the sexism that has been highlighted by other female riders, and also claims she was discriminated against because of her age.\n",
      "\"I can certainly relate to the bullying,\" she said. \"For me personally, I felt it was more ageism - being a little bit older than my team-mates, it didn't seem to be something that the staff necessarily wanted for our team in 2012.\n",
      "\"They didn't care about what happened to me afterwards. I never heard another thing from them.\n",
      "\"After six years of constantly medalling at World Cups, World Championships, nationals, both on the track and on the road, they discarded me in a very undignified way from the team, which I don't feel was right.\"\n",
      "Fourteen-time Paralympic gold medallist Dame Sarah Storey told BBC Sport that elite level sport in Britain is \"cut-throat\" but there are \"no excuses for crossing that line\" into bullying.\n",
      "Asked about the balance between winning and athlete welfare, the 39-year-old replied: \"It's a really difficult question because you have to be a human being, you have to allow for people to make mistakes. But the currency is race wins, the currency is gold medals.\n",
      "\"It's not an excuse but you have to have a thick skin in sport, you have to be able to take the rough with the smooth because of the racing that you go through.\n",
      "\"But there are no excuses for crossing that line, and if those lines have been crossed they will be found out and they'll be dealt with.\"\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.tokenizer.decode(te1.input_ids,clean_up_tokenization_spaces=True,skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 610)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [35:53<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "import tqdm\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=12,\n",
    "                              sampler= SequentialSampler(test_dataset),\n",
    "                              collate_fn= SmartCollator(\n",
    "            pad_token_id=train_dataset.tokenizer.pad_token_id, max_len=800,\n",
    "            context_max_len=300,\n",
    "            context_sampling_bounds=(0.1, 0.45)\n",
    "        )\n",
    "                              )\n",
    "\n",
    "output_summaries2 =[]\n",
    "for batch in tqdm.tqdm(test_data_loader):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    seq_len = b_input_ids.shape[1]\n",
    "    \n",
    "    boundary_mask =  batch.get(\"boundary\",)\n",
    "    bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            max_length=140)\n",
    "    sentences = test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)\n",
    "    output_summaries2+=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 470)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#context_enforcement/data/common.py\n",
    "from context_enforcement.data.common import write_to_file\n",
    "write_to_file(output_summaries2[:len(test_dataset)], \"outputs/attempt1/t5_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [tokenizer.decode(c.labels,clean_up_tokenization_spaces=True,skip_special_tokens=True) for c in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Musician Koffi Olomide has been taken into custody in the Democratic Republic of Congo, days after he was deported from Kenya for allegedly kicking one of his dancers at an airport in Nairobi.',\n",
       " 'Congolese singer Olomide has been detained by police in the Democratic Republic of Congo after he was filmed assaulting a dancer.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[690],output_summaries2[690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metrics = evaluate.combine(['bleu','meteor',\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.10229038581666045,\n",
       " 'precisions': [0.3928173359103983,\n",
       "  0.14167718301959548,\n",
       "  0.07141177501233384,\n",
       "  0.039391875456093406],\n",
       " 'brevity_penalty': 0.9144670367817925,\n",
       " 'length_ratio': 0.9179248030409859,\n",
       " 'translation_length': 239549,\n",
       " 'reference_length': 260968,\n",
       " 'meteor': 0.32270879955148984,\n",
       " 'rouge1': 0.3723759012640018,\n",
       " 'rouge2': 0.15332988714608908,\n",
       " 'rougeL': 0.29900944005710295,\n",
       " 'rougeLsum': 0.2990488997322651}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4 = metrics.compute(predictions=output_summaries2,references=targets)\n",
    "scores4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nlplab/hdd2/Laith/jojos_work/context-enforcement/inference_baseline_t5.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blaith_server/home/nlplab/hdd2/Laith/jojos_work/context-enforcement/inference_baseline_t5.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m scores\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.08190144303338862,\n",
       " 'precisions': [0.39916558741021463,\n",
       "  0.13484576712224944,\n",
       "  0.06490478666102122,\n",
       "  0.03461400195153127],\n",
       " 'brevity_penalty': 0.7810193894484071,\n",
       " 'length_ratio': 0.8018247587808374,\n",
       " 'translation_length': 209249,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.28558238733460617,\n",
       " 'rouge1': 0.34475899874216664,\n",
       " 'rouge2': 0.13444853158452552,\n",
       " 'rougeL': 0.279360220097055,\n",
       " 'rougeLsum': 0.27932587220115546}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.08004077243766287,\n",
       " 'precisions': [0.39668147524032016,\n",
       "  0.1326978952590178,\n",
       "  0.06327462141553002,\n",
       "  0.03340461786535229],\n",
       " 'brevity_penalty': 0.7793378458842136,\n",
       " 'length_ratio': 0.8004414368155238,\n",
       " 'translation_length': 208888,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.28278544337874784,\n",
       " 'rouge1': 0.34230456443341767,\n",
       " 'rouge2': 0.13258619227287083,\n",
       " 'rougeL': 0.27739509890876,\n",
       " 'rougeLsum': 0.2774115466752346}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11312982003210258,\n",
       " 'precisions': [0.43367935409457903,\n",
       "  0.170965703452634,\n",
       "  0.09041255378385218,\n",
       "  0.051220088499377064],\n",
       " 'brevity_penalty': 0.8310753377221631,\n",
       " 'length_ratio': 0.8438570541756397,\n",
       " 'translation_length': 220218,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.3379248069191188,\n",
       " 'rouge1': 0.3902876664001791,\n",
       " 'rouge2': 0.17375564911724187,\n",
       " 'rougeL': 0.3204999917403623,\n",
       " 'rougeLsum': 0.3205687202163906}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11146082917260128,\n",
       " 'precisions': [0.4317755562839142,\n",
       "  0.1685194251648763,\n",
       "  0.08904895332074475,\n",
       "  0.050525125222168364],\n",
       " 'brevity_penalty': 0.8286310630214466,\n",
       " 'length_ratio': 0.8417648276020631,\n",
       " 'translation_length': 219672,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.334512237524537,\n",
       " 'rouge1': 0.3879467178046803,\n",
       " 'rouge2': 0.17119907794549927,\n",
       " 'rougeL': 0.31801217096792334,\n",
       " 'rougeLsum': 0.31809606706132665}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
