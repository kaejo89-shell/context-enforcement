{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/anaconda3/envs/development/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 1376\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Global seed set to 1376\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Optional\n",
    "sys.path.append('../')\n",
    "\n",
    "#https://github.com/amazon-science/efficient-longdoc-classification\n",
    "from functools import partial\n",
    "import nltk\n",
    "import pickle as pk\n",
    "import torch\n",
    "from context_enforcement.models.context_enforcer import compute_context_boundary\n",
    "from context_enforcement.trainers.train_bart3 import model_init\n",
    "from context_enforcement.data.common import create_text_tokenizer, SmartCollator\n",
    "from context_enforcement.trainers.common import get_dataset_specified_tasks\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import sys\n",
    "import os\n",
    "seed_everything(1376)\n",
    "\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig\n",
    "bart_config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "tokenizer = create_text_tokenizer('facebook/bart-base')\n",
    "\n",
    "task_dataset_gen = get_dataset_specified_tasks('xsum')\n",
    "\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "test_dataset = None\n",
    "if task_dataset_gen is not None:\n",
    "    raw_dataset = task_dataset_gen(tokenizer=tokenizer, )\n",
    "    train_dataset = raw_dataset['train']\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    test_dataset = raw_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from transformers import BartConfig\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.models.bart.modeling_bart import BartAttention\n",
    "\n",
    "from context_enforcement.models.context_enforcer import split_contexts_with_boundary\n",
    "\n",
    "\n",
    "class BartContextEnforcerLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config: BartConfig,\n",
    "                 is_normal_layer: bool = False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        \n",
    "        # Check if use_random_restriction is true or false\n",
    "        self.context_enforcer = None\n",
    "        self.context_enforcer_layer_norm = None\n",
    "        self.self_attn = BartAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.encoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # Context modeling\n",
    "        dim = self.embed_dim\n",
    "        self._wlc = Linear(dim, dim)\n",
    "        self._wrc = Linear(dim, dim)\n",
    "        self._wfc = Linear(dim, dim)\n",
    "\n",
    "    def _compute_context(self, hidden_states: torch.FloatTensor,\n",
    "                         attention_mask: torch.FloatTensor,\n",
    "                         context_boundary: Tuple[int, int],\n",
    "                         layer_head_mask: torch.FloatTensor,\n",
    "                         output_attentions: Optional[bool] = False, ):\n",
    "\n",
    "        [left_context, focus_context, right_context] = split_contexts_with_boundary(\n",
    "            hidden_states,\n",
    "            context_boundary,\n",
    "        )\n",
    "\n",
    "        boundary_start, boundary_end = context_boundary\n",
    "        fc_seq_len = focus_context.shape[1]\n",
    "        lc_seq_len = left_context.shape[1]\n",
    "        rc_seq_len = right_context.shape[1]\n",
    "        left_attention = None\n",
    "        right_attention = None\n",
    "        focus_attention_mask = None\n",
    "        if attention_mask is not None:\n",
    "            left_attention = attention_mask[:, :, :boundary_start, boundary_start:boundary_end]\n",
    "            right_attention = attention_mask[:, :, boundary_end:, boundary_start:boundary_end]\n",
    "            zc= torch.zeros(size=(left_context.shape[0], 1, fc_seq_len, 1))\n",
    "            focus_attention_mask = torch.concat([attention_mask[:, :, boundary_start:boundary_end, :boundary_start],\n",
    "                                                 zc,\n",
    "                                                 attention_mask[:, :, boundary_start:boundary_end, boundary_end:]\n",
    "                                                 ], dim=3)\n",
    "\n",
    "        # Compute the context focus attention\n",
    "        left_focus, left_focus_attention, _ = self.self_attn(\n",
    "            hidden_states=left_context,\n",
    "            key_value_states=focus_context,\n",
    "            attention_mask=left_attention,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        right_focus, right_focus_attention, _ = self.self_attn(\n",
    "            hidden_states=right_context,\n",
    "            key_value_states=focus_context,\n",
    "            attention_mask=right_attention,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        right_focus = self.activation_fn(self._wrc(right_focus)) + right_context\n",
    "        left_focus = self.activation_fn(self._wlc(left_focus)) + left_context\n",
    "\n",
    "        boundary = torch.zeros(\n",
    "            (left_focus.shape[0], 1, left_focus.shape[-1]), device=left_focus.device\n",
    "        )\n",
    "        context = torch.concat([left_focus, boundary, right_focus], dim=1)\n",
    "\n",
    "        focus_rep, focus_attention,_ = self.self_attn(hidden_states=focus_context,\n",
    "                                                     key_value_states=context,\n",
    "                                                     attention_mask=focus_attention_mask,\n",
    "                                                     layer_head_mask=layer_head_mask,\n",
    "                                                     output_attentions=output_attentions, )\n",
    "        focus_rep = self.activation_fn(self._wfc(focus_rep)) + focus_context\n",
    "\n",
    "        # Stitch the full reps together\n",
    "        full_rep = torch.concat([left_focus, focus_rep, right_focus], dim=1)\n",
    "        atten_weights = [left_focus_attention, focus_attention, right_focus_attention]\n",
    "\n",
    "        return full_rep, atten_weights\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.FloatTensor,\n",
    "            attention_mask: torch.FloatTensor,\n",
    "            context_boundary: Tuple[int, int],\n",
    "            layer_head_mask: torch.FloatTensor,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states, attn_weights = self._compute_context(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            context_boundary=context_boundary\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if hidden_states.dtype == torch.float16 and (\n",
    "                torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
    "        ):\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states,\n",
    "                                        min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_layer = BartContextEnforcerLayer(config=bart_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "te1= test_dataset[692]\n",
    "b_input_ids = te1.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = te1.attention_mask.view(1, -1).to(device)\n",
    "attention_mask = _expand_mask(b_input_mask, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#seed_everything(1376)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m boundary_sample \u001b[39m=\u001b[39m  (\u001b[39m0.15\u001b[39m, \u001b[39m0.65\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m seq_len \u001b[39m=\u001b[39m te1\u001b[39m.\u001b[39minput_ids[:\u001b[39m1024\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m boundary_width \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.33\u001b[39m\u001b[39m*\u001b[39mseq_len) \n\u001b[1;32m      6\u001b[0m context_boundary \u001b[39m=\u001b[39m compute_context_boundary(seq_len,\n\u001b[1;32m      7\u001b[0m                                             context_sampling_bounds\u001b[39m=\u001b[39mboundary_sample,\n\u001b[1;32m      8\u001b[0m                                             context_max_len\u001b[39m=\u001b[39mboundary_width)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'te1' is not defined"
     ]
    }
   ],
   "source": [
    "#seed_everything(1376)\n",
    "boundary_sample =  (0.15, 0.65)\n",
    "seq_len = te1.input_ids[:1024].shape[0]\n",
    "\n",
    "boundary_width = int(0.33*seq_len) \n",
    "context_boundary = compute_context_boundary(seq_len,\n",
    "                                            context_sampling_bounds=boundary_sample,\n",
    "                                            context_max_len=boundary_width)\n",
    "context_boundary,seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "601-432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed= torch.rand(size=(1,seq_len,bart_config.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 432, 169])\n",
      "torch.Size([1, 1, 169, 1])\n",
      "ZZZZZZZZzzz\n",
      "torch.Size([1, 1, 169, 432])\n",
      "torch.Size([1, 1, 169, 207])\n"
     ]
    }
   ],
   "source": [
    "boutput= bart_layer(embed,attention_mask,context_boundary,layer_head_mask=None,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnd_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
