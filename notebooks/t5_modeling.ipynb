{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1376\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from transformers.utils import logging\n",
    "from typing import Optional, Tuple, Union\n",
    "import warnings\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers.models.t5.modeling_t5 import (assert_device_map,T5EncoderModel,\n",
    "                                                T5LayerSelfAttention,T5LayerFF,\n",
    "                                                T5Config,T5PreTrainedModel,\n",
    "                                                T5LayerNorm,get_device_map,\n",
    "                                                add_start_docstrings,\n",
    "                                                DEPARALLELIZE_DOCSTRING,\n",
    "                                                replace_return_docstrings,\n",
    "                                                add_start_docstrings_to_model_forward,\n",
    "                                                T5_INPUTS_DOCSTRING,\n",
    "                                                T5_START_DOCSTRING,\n",
    "                                                \n",
    "                                                __HEAD_MASK_WARNING_MSG,\n",
    "                                                _CONFIG_FOR_DOC,\n",
    "                                                T5Stack,\n",
    "                                                PARALLELIZE_DOCSTRING)\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput,BaseModelOutput,Seq2SeqModelOutput,BaseModelOutputWithPastAndCrossAttentions\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "from context_enforcement.models.common import EncoderOutputs, Seq2SeqLMOutputBoundary, T5ModelOutput\n",
    "logger = logging.get_logger(__name__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "seed_everything(1376)\n",
    "\n",
    "from context_enforcement.models.context_enforcer import ContextEnforcement, compute_context_boundary\n",
    "\n",
    "\n",
    "class T5ContextEnforcedEncoderBlock(nn.Module):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__()\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n",
    "        self.layer.append(T5LayerFF(config))\n",
    "        self.dropout = config.dropout_rate\n",
    "        \n",
    "        self.context_enforcer = ContextEnforcement(config.d_model,\n",
    "                                                   num_heads=config.num_heads)\n",
    "        self.context_enforcer_layer_norm = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        context_boundary: Tuple[int, int]=None,\n",
    "        position_bias=None,\n",
    "        \n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        encoder_decoder_position_bias=None,\n",
    "        layer_head_mask=None,\n",
    "        cross_attn_layer_head_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            if not self.is_decoder:\n",
    "                Logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n",
    "            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n",
    "\n",
    "            if len(past_key_value) != expected_num_past_key_values:\n",
    "                raise ValueError(\n",
    "                    f\"There should be {expected_num_past_key_values} past states. \"\n",
    "                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n",
    "                    f\"Got {len(past_key_value)} past key / value states\"\n",
    "                )\n",
    "\n",
    "            self_attn_past_key_value = past_key_value[:2]\n",
    "            cross_attn_past_key_value = past_key_value[2:]\n",
    "        else:\n",
    "            self_attn_past_key_value, cross_attn_past_key_value = None, None\n",
    "\n",
    "        self_attention_outputs = self.layer[0](\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_bias=position_bias,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states, present_key_value_state = self_attention_outputs[:2]\n",
    "        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n",
    "\n",
    "        # clamp inf values to enable fp16 training\n",
    "        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        \n",
    "        # Put the context enforcer here\n",
    "        residual = hidden_states\n",
    "        hidden_states, _ = self.context_enforcer(hidden_states,context_boundary,output_attentions)\n",
    "        hidden_states = nn.functional.dropout(hidden_states[1], p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        hidden_states = self.context_enforcer_layer_norm(hidden_states)\n",
    "        \n",
    "        # clamp inf values to enable fp16 training\n",
    "        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "        \n",
    "\n",
    "        # Apply Feed Forward layer\n",
    "        hidden_states = self.layer[-1](hidden_states)\n",
    "\n",
    "        # clamp inf values to enable fp16 training\n",
    "        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = outputs + (present_key_value_state,) + attention_outputs\n",
    "        else:\n",
    "            outputs = outputs + attention_outputs\n",
    "\n",
    "        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n",
    "    \n",
    "\n",
    "\n",
    "class T5ContextEnforcementEncoderStack(T5PreTrainedModel):\n",
    "    def __init__(self, config, embed_tokens=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "        self.block = nn.ModuleList(\n",
    "            [T5ContextEnforcedEncoderBlock(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n",
    "        )\n",
    "        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.gradient_checkpointing = False\n",
    "        \n",
    "        self.context_max_len= config.context_max_len\n",
    "        self.context_sampling_bounds = config.context_sampling_bounds\n",
    "\n",
    "    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n",
    "    def parallelize(self, device_map=None):\n",
    "        # Check validity of device_map\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.block), range(torch.cuda.device_count())) if device_map is None else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.block))\n",
    "        self.model_parallel = True\n",
    "        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n",
    "        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n",
    "        # Load onto devices\n",
    "        for k, v in self.device_map.items():\n",
    "            for layer in v:\n",
    "                cuda_device = \"cuda:\" + str(k)\n",
    "                self.block[layer] = self.block[layer].to(cuda_device)\n",
    "\n",
    "        # Set embed_tokens to first layer\n",
    "        self.embed_tokens = self.embed_tokens.to(self.first_device)\n",
    "        # Set final layer norm to last device\n",
    "        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n",
    "\n",
    "    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n",
    "    def deparallelize(self):\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        self.first_device = \"cpu\"\n",
    "        self.last_device = \"cpu\"\n",
    "        for i in range(len(self.block)):\n",
    "            self.block[i] = self.block[i].to(\"cpu\")\n",
    "        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n",
    "        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.embed_tokens = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        context_boundary: Tuple[int, int] = None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        # Model parallel\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.first_device)\n",
    "            self.embed_tokens = self.embed_tokens.to(self.first_device)\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            err_msg_prefix = \"\"\n",
    "            raise ValueError(\n",
    "                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            err_msg_prefix =  \"\"\n",
    "            raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        # required mask seq length can be calculated via length of past\n",
    "        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n",
    "\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n",
    "\n",
    "        # initialize past_key_values with `None` if past does not exist\n",
    "        if past_key_values is None:\n",
    "            past_key_values = [None] * len(self.block)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n",
    "        cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n",
    "        present_key_value_states = () if use_cache else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        all_cross_attentions =  None\n",
    "        position_bias = None\n",
    "        encoder_decoder_position_bias = None\n",
    "\n",
    "        hidden_states = self.dropout(inputs_embeds)\n",
    "        \n",
    "        if context_boundary is None:\n",
    "            context_boundary = compute_context_boundary(seq_length,\n",
    "                                                        context_max_len=self.context_max_len,\n",
    "                                                        context_sampling_bounds= self.context_sampling_bounds)\n",
    "\n",
    "        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n",
    "            layer_head_mask = head_mask[i]\n",
    "            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n",
    "            # Model parallel\n",
    "            if self.model_parallel:\n",
    "                torch.cuda.set_device(hidden_states.device)\n",
    "                # Ensure that attention_mask is always on the same device as hidden_states\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = attention_mask.to(hidden_states.device)\n",
    "                if position_bias is not None:\n",
    "                    position_bias = position_bias.to(hidden_states.device)\n",
    "                if encoder_hidden_states is not None:\n",
    "                    encoder_hidden_states = encoder_hidden_states.to(hidden_states.device)\n",
    "                if encoder_extended_attention_mask is not None:\n",
    "                    encoder_extended_attention_mask = encoder_extended_attention_mask.to(hidden_states.device)\n",
    "                if encoder_decoder_position_bias is not None:\n",
    "                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(hidden_states.device)\n",
    "                if layer_head_mask is not None:\n",
    "                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n",
    "                if cross_attn_layer_head_mask is not None:\n",
    "                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(hidden_states.device)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return tuple(module(*inputs, use_cache, output_attentions))\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    extended_attention_mask,\n",
    "                    context_boundary,\n",
    "                    position_bias,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_extended_attention_mask,\n",
    "                    encoder_decoder_position_bias,\n",
    "                    layer_head_mask,\n",
    "                    cross_attn_layer_head_mask,\n",
    "                    None,  # past_key_value is always None with gradient checkpointing\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask=extended_attention_mask,\n",
    "                    context_boundary=context_boundary,\n",
    "                    position_bias=position_bias,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_extended_attention_mask,\n",
    "                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
    "                    layer_head_mask=layer_head_mask,\n",
    "                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n",
    "                    past_key_value=past_key_value,\n",
    "                    use_cache=use_cache,\n",
    "                    output_attentions=output_attentions,\n",
    "                )\n",
    "\n",
    "            # layer_outputs is a tuple with:\n",
    "            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n",
    "            if use_cache is False:\n",
    "                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n",
    "\n",
    "            hidden_states, present_key_value_state = layer_outputs[:2]\n",
    "\n",
    "            # We share the position biases between the layers - the first layer store them\n",
    "            # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n",
    "            # (cross-attention position bias), (cross-attention weights)\n",
    "            position_bias = layer_outputs[2]\n",
    "            if self.is_decoder and encoder_hidden_states is not None:\n",
    "                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n",
    "            # append next layer key value states\n",
    "            if use_cache:\n",
    "                present_key_value_states = present_key_value_states + (present_key_value_state,)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[3],)\n",
    "                if self.is_decoder:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n",
    "\n",
    "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "            if self.model_parallel:\n",
    "                for k, v in self.device_map.items():\n",
    "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)#\n",
    "        \n",
    "        \n",
    "        hidden_states = hidden_states[:, context_boundary[0]:context_boundary[1], :]\n",
    "        context_len = hidden_states.shape[1]\n",
    "        \n",
    "        \n",
    "        batch_encoder_attention_masks = torch.ones((hidden_states.shape[0],\n",
    "                                                    context_len),dtype=torch.long,\n",
    "                                                       device= hidden_states.device)\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    present_key_value_states,\n",
    "                    all_hidden_states,\n",
    "                    all_attentions,\n",
    "                    all_cross_attentions,\n",
    "                    context_boundary,\n",
    "            batch_encoder_attention_masks\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return T5ModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=present_key_value_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "            context_boundary=context_boundary,\n",
    "            cleaned_mask = batch_encoder_attention_masks\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "@add_start_docstrings(\"\"\"T5 Model with a `language modeling` head on top.\"\"\", T5_START_DOCSTRING)\n",
    "class T5ContextEnforcementSeqModel(T5PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [\n",
    "        r\"encoder.embed_tokens.weight\",\n",
    "        r\"decoder.embed_tokens.weight\",\n",
    "        r\"lm_head.weight\",\n",
    "    ]\n",
    "    _keys_to_ignore_on_load_unexpected = [\n",
    "        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "\n",
    "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        encoder_config = copy.deepcopy(config)\n",
    "        encoder_config.is_decoder = False\n",
    "        encoder_config.use_cache = False\n",
    "        encoder_config.is_encoder_decoder = False\n",
    "        self.encoder = T5ContextEnforcementEncoderStack(encoder_config, self.shared)\n",
    "\n",
    "        decoder_config = copy.deepcopy(config)\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.is_encoder_decoder = False\n",
    "        decoder_config.num_layers = config.num_decoder_layers\n",
    "        self.decoder = T5Stack(decoder_config, self.shared)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "        self.context_max_len= config.context_max_len\n",
    "        self.context_sampling_bounds = config.context_sampling_bounds\n",
    "\n",
    "    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.encoder.block))\n",
    "        self.encoder.parallelize(self.device_map)\n",
    "        self.decoder.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.decoder.first_device)\n",
    "        self.model_parallel = True\n",
    "\n",
    "    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n",
    "    def deparallelize(self):\n",
    "        self.encoder.deparallelize()\n",
    "        self.decoder.deparallelize()\n",
    "        self.encoder = self.encoder.to(\"cpu\")\n",
    "        self.decoder = self.decoder.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.shared = new_embeddings\n",
    "        self.encoder.set_input_embeddings(new_embeddings)\n",
    "        self.decoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "    \n",
    "    \n",
    "    \n",
    "    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        context_boundary: Tuple[int, int] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n",
    "            labels in `[0, ..., config.vocab_size]`\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "        >>> # training\n",
    "        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model(input_ids=input_ids, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "\n",
    "        >>> # inference\n",
    "        >>> input_ids = tokenizer(\n",
    "        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    "        ... ).input_ids  # Batch size 1\n",
    "        >>> outputs = model.generate(input_ids)\n",
    "        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        >>> # studies have shown that owning a dog is good for you.\n",
    "        ```\"\"\"\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            seq_len= input_ids.shape[-1]\n",
    "        else:\n",
    "            seq_len=encoder_outputs[0].shape[1]\n",
    "        \n",
    "        if context_boundary is None:\n",
    "            context_boundary = compute_context_boundary(seq_len,\n",
    "                                                        context_max_len=self.context_max_len,\n",
    "                                                        context_sampling_bounds= self.context_sampling_bounds)\n",
    "\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            # Convert encoder inputs in embeddings if needed\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                context_boundary = context_boundary,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, EncoderOutputs):\n",
    "            batch_attention = None\n",
    "            if attention_mask is not None:\n",
    "                batch_attention = attention_mask[:,context_boundary[0]:context_boundary[1]]\n",
    "                \n",
    "            \n",
    "            encoder_outputs = EncoderOutputs(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "                context_boundary=encoder_outputs[5] if len(encoder_outputs) > 5 else context_boundary,\n",
    "                cleaned_mask=encoder_outputs[6] if len(encoder_outputs) > 6 else batch_attention,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            # get decoder inputs from shifting lm labels to the right\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "        \n",
    "        context_len = hidden_states.shape[1]\n",
    "        batch_encoder_attention_masks = torch.ones((hidden_states.shape[0],\n",
    "                                                    context_len),dtype=torch.long,\n",
    "                                                       device= hidden_states.device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=batch_encoder_attention_masks,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = decoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.encoder.first_device)\n",
    "            self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
    "            sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
    "\n",
    "        if self.config.tie_word_embeddings:\n",
    "            # Rescale output before projecting on vocab\n",
    "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutputBoundary(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "            context_boundary=context_boundary\n",
    "        )\n",
    "    \n",
    "    def strip_attention_mask(self,\n",
    "                             context_boundary,\n",
    "                             attention_mask):\n",
    "        attn= torch.ones_like(attention_mask)\n",
    "        batched_attention =attn[:, context_boundary[0]:context_boundary[1]]\n",
    "        return batched_attention\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "        \n",
    "        \n",
    "        factor = encoder_outputs[0].shape[0] // encoder_outputs.cleaned_mask.shape[0]\n",
    "\n",
    "        attention_mask = encoder_outputs.cleaned_mask.repeat_interleave(factor, dim=0)\n",
    "        \n",
    "        separation_points = encoder_outputs.context_boundary\n",
    "        \n",
    "        if encoder_outputs[0].shape[:-1] != attention_mask.shape:\n",
    "            attention_mask = self.strip_attention_mask(separation_points, attention_mask)\n",
    "\n",
    "        return {\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return self._shift_right(labels)\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        # if decoder past is not included in output\n",
    "        # speedy decoding is disabled and no need to reorder\n",
    "        if past is None:\n",
    "            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n",
    "            return past\n",
    "\n",
    "        reordered_decoder_past = ()\n",
    "        for layer_past_states in past:\n",
    "            # get the correct batch idx from layer past batch dim\n",
    "            # batch dim of `past` is at 2nd position\n",
    "            reordered_layer_past_states = ()\n",
    "            for layer_past_state in layer_past_states:\n",
    "                # need to set correct `past` for each of the four key / value states\n",
    "                reordered_layer_past_states = reordered_layer_past_states + (\n",
    "                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n",
    "                )\n",
    "\n",
    "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
    "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
    "\n",
    "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
    "        return reordered_decoder_past\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_config = T5Config.from_pretrained('t5-base')\n",
    "t5_config.context_max_len= 30\n",
    "t5_config.context_sampling_bounds = (0.15,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_config.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ContextEnforcementSeqModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.3.context_enforcer._right_mha.out_proj.bias', 'encoder.block.1.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer._left_mha.out_proj.bias', 'encoder.block.3.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer_layer_norm.bias', 'encoder.block.0.context_enforcer._right_mha.out_proj.bias', 'encoder.block.6.context_enforcer._wlc.bias', 'encoder.block.9.context_enforcer._right_mha.in_proj_bias', 'encoder.block.11.context_enforcer._wlc.bias', 'encoder.block.10.context_enforcer._left_mha.in_proj_weight', 'encoder.block.0.context_enforcer._left_mha.out_proj.weight', 'encoder.block.4.context_enforcer._left_mha.in_proj_weight', 'encoder.block.3.context_enforcer._left_mha.in_proj_weight', 'encoder.block.9.context_enforcer._wlc.bias', 'encoder.block.4.context_enforcer._right_mha.out_proj.weight', 'encoder.block.8.context_enforcer._left_mha.out_proj.weight', 'encoder.block.11.context_enforcer._right_mha.out_proj.weight', 'encoder.block.11.context_enforcer_layer_norm.weight', 'encoder.block.8.context_enforcer._right_mha.in_proj_weight', 'encoder.block.11.context_enforcer._left_mha.in_proj_weight', 'encoder.block.2.context_enforcer._right_mha.in_proj_weight', 'encoder.block.8.context_enforcer._wlc.weight', 'encoder.block.1.context_enforcer._left_mha.in_proj_bias', 'encoder.block.4.context_enforcer._wrc.bias', 'encoder.block.5.context_enforcer_layer_norm.bias', 'encoder.block.5.context_enforcer._left_mha.out_proj.weight', 'encoder.block.6.context_enforcer._left_mha.in_proj_bias', 'encoder.block.3.context_enforcer._left_mha.out_proj.weight', 'encoder.block.3.context_enforcer._right_mha.in_proj_bias', 'encoder.block.3.context_enforcer_layer_norm.weight', 'encoder.block.9.context_enforcer._right_mha.out_proj.bias', 'encoder.block.6.context_enforcer._wrc.weight', 'encoder.block.0.context_enforcer_layer_norm.weight', 'encoder.block.9.context_enforcer._wrc.weight', 'encoder.block.2.context_enforcer._right_mha.out_proj.bias', 'encoder.block.0.context_enforcer._wrc.weight', 'encoder.block.1.context_enforcer._wlc.weight', 'encoder.block.6.context_enforcer._right_mha.in_proj_weight', 'encoder.block.9.context_enforcer._left_mha.in_proj_bias', 'encoder.block.7.context_enforcer._wlc.bias', 'encoder.block.8.context_enforcer._left_mha.out_proj.bias', 'encoder.block.2.context_enforcer._left_mha.in_proj_bias', 'encoder.block.9.context_enforcer._wrc.bias', 'encoder.block.10.context_enforcer._right_mha.out_proj.weight', 'encoder.block.9.context_enforcer._right_mha.out_proj.weight', 'encoder.block.3.context_enforcer_layer_norm.bias', 'encoder.block.8.context_enforcer._wrc.weight', 'encoder.block.10.context_enforcer._left_mha.in_proj_bias', 'encoder.block.9.context_enforcer._left_mha.in_proj_weight', 'encoder.block.10.context_enforcer._wrc.weight', 'encoder.block.0.context_enforcer._wlc.bias', 'encoder.block.3.context_enforcer._right_mha.out_proj.weight', 'encoder.block.7.context_enforcer._right_mha.out_proj.bias', 'encoder.block.9.context_enforcer_layer_norm.weight', 'encoder.block.1.context_enforcer_layer_norm.weight', 'encoder.block.11.context_enforcer._wrc.weight', 'encoder.block.5.context_enforcer._wlc.weight', 'encoder.block.10.context_enforcer._right_mha.out_proj.bias', 'encoder.block.8.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer._left_mha.out_proj.weight', 'encoder.block.6.context_enforcer_layer_norm.weight', 'encoder.block.0.context_enforcer_layer_norm.bias', 'encoder.block.5.context_enforcer._right_mha.in_proj_bias', 'encoder.block.5.context_enforcer._left_mha.in_proj_weight', 'encoder.block.8.context_enforcer._left_mha.in_proj_weight', 'encoder.block.10.context_enforcer._right_mha.in_proj_weight', 'encoder.block.11.context_enforcer._right_mha.out_proj.bias', 'encoder.block.2.context_enforcer._right_mha.out_proj.weight', 'encoder.block.2.context_enforcer_layer_norm.bias', 'encoder.block.2.context_enforcer_layer_norm.weight', 'encoder.block.11.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._right_mha.in_proj_bias', 'encoder.block.8.context_enforcer._right_mha.out_proj.bias', 'encoder.block.6.context_enforcer._wrc.bias', 'encoder.block.3.context_enforcer._left_mha.in_proj_bias', 'encoder.block.8.context_enforcer._wrc.bias', 'encoder.block.5.context_enforcer._left_mha.out_proj.bias', 'encoder.block.6.context_enforcer._wlc.weight', 'encoder.block.7.context_enforcer_layer_norm.weight', 'encoder.block.11.context_enforcer._left_mha.out_proj.bias', 'encoder.block.2.context_enforcer._wrc.bias', 'encoder.block.11.context_enforcer._left_mha.out_proj.weight', 'encoder.block.11.context_enforcer_layer_norm.bias', 'encoder.block.4.context_enforcer._left_mha.out_proj.weight', 'encoder.block.6.context_enforcer._right_mha.out_proj.weight', 'encoder.block.8.context_enforcer._right_mha.in_proj_bias', 'encoder.block.10.context_enforcer._left_mha.out_proj.weight', 'encoder.block.3.context_enforcer._wlc.bias', 'encoder.block.7.context_enforcer._right_mha.in_proj_weight', 'encoder.block.0.context_enforcer._wrc.bias', 'encoder.block.11.context_enforcer._wrc.bias', 'encoder.block.2.context_enforcer._wlc.bias', 'encoder.block.7.context_enforcer._wrc.bias', 'encoder.block.1.context_enforcer._left_mha.in_proj_weight', 'encoder.block.7.context_enforcer._left_mha.in_proj_weight', 'encoder.block.5.context_enforcer._right_mha.in_proj_weight', 'encoder.block.4.context_enforcer._left_mha.in_proj_bias', 'encoder.block.4.context_enforcer._wrc.weight', 'encoder.block.7.context_enforcer._wrc.weight', 'encoder.block.4.context_enforcer._wlc.weight', 'encoder.block.1.context_enforcer_layer_norm.bias', 'encoder.block.6.context_enforcer._left_mha.in_proj_weight', 'encoder.block.3.context_enforcer._wrc.bias', 'encoder.block.8.context_enforcer_layer_norm.bias', 'encoder.block.7.context_enforcer_layer_norm.bias', 'encoder.block.1.context_enforcer._right_mha.in_proj_bias', 'encoder.block.4.context_enforcer._right_mha.out_proj.bias', 'encoder.block.6.context_enforcer._right_mha.in_proj_bias', 'encoder.block.5.context_enforcer_layer_norm.weight', 'encoder.block.7.context_enforcer._right_mha.in_proj_bias', 'encoder.block.7.context_enforcer._right_mha.out_proj.weight', 'encoder.block.7.context_enforcer._left_mha.out_proj.bias', 'encoder.block.5.context_enforcer._right_mha.out_proj.bias', 'encoder.block.4.context_enforcer_layer_norm.weight', 'encoder.block.2.context_enforcer._left_mha.in_proj_weight', 'encoder.block.4.context_enforcer._right_mha.in_proj_weight', 'encoder.block.4.context_enforcer._left_mha.out_proj.bias', 'encoder.block.0.context_enforcer._left_mha.in_proj_bias', 'encoder.block.10.context_enforcer._wlc.bias', 'encoder.block.1.context_enforcer._wrc.weight', 'encoder.block.2.context_enforcer._right_mha.in_proj_bias', 'encoder.block.5.context_enforcer._right_mha.out_proj.weight', 'encoder.block.3.context_enforcer._left_mha.out_proj.bias', 'encoder.block.0.context_enforcer._right_mha.in_proj_bias', 'encoder.block.11.context_enforcer._left_mha.in_proj_bias', 'encoder.block.3.context_enforcer._wlc.weight', 'encoder.block.3.context_enforcer._wrc.weight', 'encoder.block.0.context_enforcer._wlc.weight', 'encoder.block.8.context_enforcer._left_mha.in_proj_bias', 'encoder.block.6.context_enforcer_layer_norm.bias', 'encoder.block.1.context_enforcer._wrc.bias', 'encoder.block.4.context_enforcer._right_mha.in_proj_bias', 'encoder.block.5.context_enforcer._wrc.bias', 'encoder.block.9.context_enforcer_layer_norm.bias', 'encoder.block.5.context_enforcer._wrc.weight', 'encoder.block.1.context_enforcer._wlc.bias', 'encoder.block.1.context_enforcer._right_mha.in_proj_weight', 'encoder.block.11.context_enforcer._right_mha.in_proj_bias', 'encoder.block.1.context_enforcer._left_mha.out_proj.bias', 'encoder.block.0.context_enforcer._left_mha.in_proj_weight', 'encoder.block.1.context_enforcer._right_mha.out_proj.bias', 'encoder.block.0.context_enforcer._left_mha.out_proj.bias', 'encoder.block.7.context_enforcer._left_mha.in_proj_bias', 'encoder.block.6.context_enforcer._left_mha.out_proj.bias', 'encoder.block.8.context_enforcer_layer_norm.weight', 'encoder.block.1.context_enforcer._left_mha.out_proj.weight', 'encoder.block.4.context_enforcer._wlc.bias', 'encoder.block.8.context_enforcer._wlc.bias', 'encoder.block.2.context_enforcer._wlc.weight', 'encoder.block.10.context_enforcer._left_mha.out_proj.bias', 'encoder.block.10.context_enforcer_layer_norm.weight', 'encoder.block.9.context_enforcer._left_mha.out_proj.bias', 'encoder.block.9.context_enforcer._wlc.weight', 'encoder.block.2.context_enforcer._wrc.weight', 'encoder.block.7.context_enforcer._wlc.weight', 'encoder.block.9.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._wlc.weight', 'encoder.block.6.context_enforcer._left_mha.out_proj.weight', 'encoder.block.0.context_enforcer._right_mha.in_proj_weight', 'encoder.block.10.context_enforcer._wrc.bias', 'encoder.block.11.context_enforcer._wlc.weight', 'encoder.block.5.context_enforcer._left_mha.in_proj_bias', 'encoder.block.0.context_enforcer._right_mha.out_proj.weight', 'encoder.block.9.context_enforcer._left_mha.out_proj.weight', 'encoder.block.7.context_enforcer._left_mha.out_proj.weight', 'encoder.block.4.context_enforcer_layer_norm.bias', 'encoder.block.6.context_enforcer._right_mha.out_proj.bias', 'encoder.block.5.context_enforcer._wlc.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ContextEnforcementEncoderStack: ['encoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.final_layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'shared.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight']\n",
      "- This IS expected if you are initializing T5ContextEnforcementEncoderStack from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ContextEnforcementEncoderStack from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5ContextEnforcementEncoderStack were not initialized from the model checkpoint at t5-base and are newly initialized: ['block.7.context_enforcer._right_mha.in_proj_bias', 'block.1.layer.0.SelfAttention.q.weight', 'block.1.context_enforcer._left_mha.out_proj.weight', 'block.10.context_enforcer._wlc.weight', 'block.10.context_enforcer._left_mha.in_proj_bias', 'block.1.context_enforcer._wlc.weight', 'block.5.context_enforcer._wrc.weight', 'block.8.layer.0.layer_norm.weight', 'block.9.context_enforcer._right_mha.out_proj.weight', 'block.2.context_enforcer._right_mha.out_proj.weight', 'block.8.context_enforcer._left_mha.in_proj_weight', 'block.8.context_enforcer._right_mha.in_proj_bias', 'block.3.context_enforcer._left_mha.out_proj.bias', 'block.4.layer.1.DenseReluDense.wo.weight', 'block.4.context_enforcer_layer_norm.bias', 'block.10.layer.0.SelfAttention.q.weight', 'block.0.layer.1.layer_norm.weight', 'block.4.context_enforcer._wlc.bias', 'block.7.context_enforcer._right_mha.out_proj.bias', 'block.6.context_enforcer._wlc.bias', 'block.3.context_enforcer_layer_norm.bias', 'block.5.context_enforcer._wlc.weight', 'block.6.context_enforcer._left_mha.out_proj.weight', 'block.9.context_enforcer._left_mha.in_proj_weight', 'block.4.layer.0.SelfAttention.q.weight', 'block.3.context_enforcer._left_mha.out_proj.weight', 'block.9.layer.1.DenseReluDense.wo.weight', 'block.11.context_enforcer._wrc.bias', 'block.10.layer.1.layer_norm.weight', 'block.1.layer.0.SelfAttention.k.weight', 'block.6.context_enforcer._wrc.bias', 'block.7.layer.0.SelfAttention.k.weight', 'block.0.context_enforcer._right_mha.out_proj.weight', 'block.0.context_enforcer._wlc.weight', 'block.3.layer.0.SelfAttention.o.weight', 'block.4.context_enforcer._right_mha.in_proj_weight', 'block.8.layer.1.layer_norm.weight', 'block.2.layer.0.SelfAttention.q.weight', 'block.11.context_enforcer._left_mha.in_proj_bias', 'block.11.context_enforcer._right_mha.in_proj_bias', 'block.11.context_enforcer_layer_norm.bias', 'block.9.context_enforcer._left_mha.in_proj_bias', 'block.1.layer.0.layer_norm.weight', 'block.7.context_enforcer._wlc.weight', 'block.0.context_enforcer._left_mha.out_proj.weight', 'block.1.layer.0.SelfAttention.v.weight', 'block.4.context_enforcer_layer_norm.weight', 'block.11.context_enforcer._left_mha.in_proj_weight', 'block.9.layer.0.SelfAttention.q.weight', 'block.0.layer.1.DenseReluDense.wo.weight', 'block.6.layer.1.DenseReluDense.wo.weight', 'final_layer_norm.weight', 'block.4.context_enforcer._left_mha.in_proj_weight', 'block.9.context_enforcer._left_mha.out_proj.weight', 'block.8.context_enforcer_layer_norm.weight', 'block.11.layer.0.SelfAttention.o.weight', 'block.8.context_enforcer._wlc.weight', 'block.5.layer.0.SelfAttention.v.weight', 'block.2.context_enforcer._left_mha.in_proj_weight', 'block.9.context_enforcer_layer_norm.bias', 'block.0.context_enforcer._right_mha.in_proj_bias', 'block.6.layer.0.SelfAttention.k.weight', 'block.6.layer.1.layer_norm.weight', 'block.6.context_enforcer._left_mha.in_proj_weight', 'block.7.context_enforcer._left_mha.out_proj.weight', 'block.1.context_enforcer._wrc.bias', 'block.4.layer.0.SelfAttention.o.weight', 'block.10.context_enforcer_layer_norm.weight', 'block.5.layer.1.layer_norm.weight', 'block.9.layer.0.SelfAttention.k.weight', 'block.11.layer.1.layer_norm.weight', 'block.3.context_enforcer._right_mha.out_proj.bias', 'block.7.context_enforcer._wrc.bias', 'block.3.layer.1.layer_norm.weight', 'block.9.context_enforcer._wlc.bias', 'block.8.layer.1.DenseReluDense.wi.weight', 'block.5.context_enforcer._wrc.bias', 'block.2.context_enforcer._wrc.weight', 'block.3.context_enforcer._right_mha.in_proj_bias', 'block.2.context_enforcer._left_mha.out_proj.weight', 'block.7.layer.0.layer_norm.weight', 'block.5.layer.0.SelfAttention.o.weight', 'block.11.context_enforcer._right_mha.out_proj.weight', 'block.4.context_enforcer._left_mha.in_proj_bias', 'block.4.context_enforcer._right_mha.in_proj_bias', 'block.4.context_enforcer._wlc.weight', 'block.4.context_enforcer._right_mha.out_proj.weight', 'block.6.context_enforcer._right_mha.in_proj_bias', 'block.3.context_enforcer._right_mha.out_proj.weight', 'block.3.context_enforcer._left_mha.in_proj_bias', 'block.2.context_enforcer_layer_norm.weight', 'block.1.context_enforcer_layer_norm.bias', 'block.6.context_enforcer_layer_norm.weight', 'block.7.layer.0.SelfAttention.v.weight', 'block.6.context_enforcer._right_mha.out_proj.weight', 'block.1.context_enforcer._right_mha.in_proj_bias', 'block.8.context_enforcer._left_mha.in_proj_bias', 'block.1.context_enforcer._wlc.bias', 'block.11.context_enforcer._wrc.weight', 'block.3.layer.0.SelfAttention.q.weight', 'block.1.context_enforcer._wrc.weight', 'block.5.layer.1.DenseReluDense.wo.weight', 'block.4.layer.1.DenseReluDense.wi.weight', 'block.10.context_enforcer._right_mha.in_proj_weight', 'block.0.context_enforcer_layer_norm.bias', 'block.1.context_enforcer._left_mha.out_proj.bias', 'block.3.context_enforcer._right_mha.in_proj_weight', 'block.8.layer.0.SelfAttention.k.weight', 'block.8.layer.1.DenseReluDense.wo.weight', 'block.9.context_enforcer._right_mha.in_proj_bias', 'block.2.layer.1.DenseReluDense.wo.weight', 'block.2.context_enforcer._right_mha.in_proj_bias', 'block.3.layer.0.SelfAttention.k.weight', 'block.3.context_enforcer._wlc.bias', 'block.6.context_enforcer._left_mha.in_proj_bias', 'block.1.context_enforcer._right_mha.in_proj_weight', 'block.2.layer.0.SelfAttention.k.weight', 'block.5.context_enforcer._right_mha.in_proj_weight', 'block.5.context_enforcer._right_mha.in_proj_bias', 'block.9.layer.1.DenseReluDense.wi.weight', 'block.1.layer.1.DenseReluDense.wi.weight', 'block.11.context_enforcer._wlc.weight', 'block.3.context_enforcer._wrc.bias', 'block.7.context_enforcer._left_mha.in_proj_bias', 'block.7.context_enforcer._left_mha.in_proj_weight', 'block.6.context_enforcer._wlc.weight', 'block.11.context_enforcer._left_mha.out_proj.bias', 'block.0.context_enforcer._right_mha.out_proj.bias', 'block.2.layer.0.SelfAttention.o.weight', 'block.2.context_enforcer._wlc.weight', 'block.6.context_enforcer_layer_norm.bias', 'block.10.context_enforcer._wrc.bias', 'block.9.context_enforcer._right_mha.in_proj_weight', 'block.10.context_enforcer_layer_norm.bias', 'block.8.context_enforcer._wlc.bias', 'block.8.context_enforcer._right_mha.in_proj_weight', 'block.7.context_enforcer._right_mha.in_proj_weight', 'block.9.layer.0.SelfAttention.o.weight', 'block.2.context_enforcer._left_mha.in_proj_bias', 'block.6.layer.0.SelfAttention.q.weight', 'block.9.context_enforcer._right_mha.out_proj.bias', 'block.10.layer.0.SelfAttention.k.weight', 'block.1.context_enforcer._left_mha.in_proj_bias', 'block.2.layer.1.layer_norm.weight', 'block.2.context_enforcer._right_mha.out_proj.bias', 'block.10.layer.1.DenseReluDense.wi.weight', 'block.0.layer.0.SelfAttention.q.weight', 'block.3.context_enforcer._left_mha.in_proj_weight', 'block.1.layer.0.SelfAttention.o.weight', 'block.4.layer.0.layer_norm.weight', 'block.1.context_enforcer_layer_norm.weight', 'block.4.context_enforcer._wrc.weight', 'block.5.layer.1.DenseReluDense.wi.weight', 'block.8.layer.0.SelfAttention.q.weight', 'block.1.context_enforcer._left_mha.in_proj_weight', 'block.10.context_enforcer._left_mha.out_proj.bias', 'block.0.context_enforcer._wrc.bias', 'block.10.layer.0.SelfAttention.o.weight', 'block.8.context_enforcer_layer_norm.bias', 'block.5.context_enforcer._wlc.bias', 'block.11.context_enforcer._right_mha.out_proj.bias', 'block.3.context_enforcer_layer_norm.weight', 'block.7.layer.1.DenseReluDense.wi.weight', 'block.5.layer.0.SelfAttention.q.weight', 'block.6.layer.0.layer_norm.weight', 'block.0.layer.0.SelfAttention.v.weight', 'block.2.context_enforcer_layer_norm.bias', 'block.9.layer.1.layer_norm.weight', 'block.1.context_enforcer._right_mha.out_proj.bias', 'block.5.context_enforcer._right_mha.out_proj.weight', 'block.2.layer.0.SelfAttention.v.weight', 'block.2.context_enforcer._wrc.bias', 'block.9.context_enforcer._wrc.weight', 'block.10.layer.1.DenseReluDense.wo.weight', 'block.5.layer.0.SelfAttention.k.weight', 'block.11.layer.0.SelfAttention.k.weight', 'block.11.layer.1.DenseReluDense.wi.weight', 'block.0.context_enforcer_layer_norm.weight', 'block.8.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.o.weight', 'block.0.context_enforcer._right_mha.in_proj_weight', 'block.2.context_enforcer._left_mha.out_proj.bias', 'block.7.layer.1.layer_norm.weight', 'block.10.context_enforcer._wrc.weight', 'block.11.layer.1.DenseReluDense.wo.weight', 'block.11.context_enforcer_layer_norm.weight', 'block.4.layer.0.SelfAttention.v.weight', 'block.3.layer.1.DenseReluDense.wo.weight', 'block.2.layer.1.DenseReluDense.wi.weight', 'block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'block.7.context_enforcer._left_mha.out_proj.bias', 'block.0.context_enforcer._left_mha.in_proj_weight', 'block.0.layer.1.DenseReluDense.wi.weight', 'block.0.layer.0.layer_norm.weight', 'block.4.context_enforcer._wrc.bias', 'block.9.layer.0.SelfAttention.v.weight', 'block.4.context_enforcer._left_mha.out_proj.weight', 'block.8.context_enforcer._left_mha.out_proj.weight', 'block.10.layer.0.SelfAttention.v.weight', 'block.0.layer.0.SelfAttention.k.weight', 'block.3.layer.1.DenseReluDense.wi.weight', 'block.3.context_enforcer._wrc.weight', 'block.7.context_enforcer._wlc.bias', 'block.3.context_enforcer._wlc.weight', 'block.6.context_enforcer._right_mha.in_proj_weight', 'block.8.context_enforcer._right_mha.out_proj.bias', 'block.10.context_enforcer._left_mha.in_proj_weight', 'block.4.layer.1.layer_norm.weight', 'block.3.layer.0.SelfAttention.v.weight', 'block.10.context_enforcer._right_mha.out_proj.bias', 'block.11.layer.0.SelfAttention.v.weight', 'block.5.context_enforcer._left_mha.in_proj_weight', 'block.11.layer.0.layer_norm.weight', 'block.8.context_enforcer._left_mha.out_proj.bias', 'block.9.layer.0.layer_norm.weight', 'block.5.context_enforcer_layer_norm.bias', 'block.2.context_enforcer._wlc.bias', 'block.7.layer.1.DenseReluDense.wo.weight', 'block.1.layer.1.layer_norm.weight', 'block.7.context_enforcer._right_mha.out_proj.weight', 'block.8.context_enforcer._right_mha.out_proj.weight', 'block.4.layer.0.SelfAttention.k.weight', 'block.4.context_enforcer._right_mha.out_proj.bias', 'block.11.context_enforcer._left_mha.out_proj.weight', 'block.11.context_enforcer._wlc.bias', 'block.3.layer.0.layer_norm.weight', 'block.10.context_enforcer._left_mha.out_proj.weight', 'block.4.context_enforcer._left_mha.out_proj.bias', 'block.11.context_enforcer._right_mha.in_proj_weight', 'block.5.layer.0.layer_norm.weight', 'block.8.context_enforcer._wrc.weight', 'block.7.context_enforcer_layer_norm.weight', 'block.9.context_enforcer._wrc.bias', 'block.5.context_enforcer._left_mha.out_proj.bias', 'block.10.context_enforcer._wlc.bias', 'block.6.context_enforcer._wrc.weight', 'block.2.context_enforcer._right_mha.in_proj_weight', 'block.5.context_enforcer._right_mha.out_proj.bias', 'block.6.layer.0.SelfAttention.o.weight', 'block.5.context_enforcer._left_mha.out_proj.weight', 'block.6.context_enforcer._left_mha.out_proj.bias', 'block.7.context_enforcer._wrc.weight', 'block.7.context_enforcer_layer_norm.bias', 'block.5.context_enforcer._left_mha.in_proj_bias', 'block.5.context_enforcer_layer_norm.weight', 'block.6.layer.1.DenseReluDense.wi.weight', 'block.7.layer.0.SelfAttention.q.weight', 'block.8.context_enforcer._wrc.bias', 'block.9.context_enforcer_layer_norm.weight', 'block.11.layer.0.SelfAttention.q.weight', 'block.0.context_enforcer._wrc.weight', 'block.1.layer.1.DenseReluDense.wo.weight', 'block.10.context_enforcer._right_mha.in_proj_bias', 'block.2.layer.0.layer_norm.weight', 'block.6.layer.0.SelfAttention.v.weight', 'block.10.layer.0.layer_norm.weight', 'block.9.context_enforcer._left_mha.out_proj.bias', 'block.8.layer.0.SelfAttention.o.weight', 'block.7.layer.0.SelfAttention.o.weight', 'block.0.context_enforcer._left_mha.out_proj.bias', 'block.0.context_enforcer._left_mha.in_proj_bias', 'block.6.context_enforcer._right_mha.out_proj.bias', 'block.9.context_enforcer._wlc.weight', 'block.10.context_enforcer._right_mha.out_proj.weight', 'block.1.context_enforcer._right_mha.out_proj.weight', 'block.0.context_enforcer._wlc.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "t5_model  = T5ContextEnforcementSeqModel.from_pretrained('t5-base',config=t5_config)\n",
    "t5_encoder = T5ContextEnforcementEncoderStack.from_pretrained('t5-base',config=t5_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_encoder.post_init()\n",
    "#t5_encoder.resize_token_embeddings(t5_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(2,6000,(2,100))\n",
    "rep = torch.rand((2,100,768))\n",
    "attn = torch.ones((2,100))\n",
    "labels= torch.randint(2,6000,(2,100))\n",
    "extended_attention_mask = attn[:, None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.resize_token_embeddings(t5_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5layer = T5ContextEnforcedEncoderBlock(config=t5_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val= t5layer(rep,\n",
    "             extended_attention_mask,(45,80),\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output = t5_model(input_ids,attn,context_boundary=(15,65),labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_output.encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv= t5_model.generate(input_ids,\n",
    "                      attention_mask=attn,\n",
    "                               num_beams=10,\n",
    "                do_sample=False,\n",
    "                num_return_sequences=1,\n",
    "                max_length=140\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
