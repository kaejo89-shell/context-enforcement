{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1376\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Global seed set to 1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from context_enforcement.models.context_enforcer import compute_context_boundary\n",
    "from context_enforcement.trainers.train_bart import model_init\n",
    "from context_enforcement.data.common import create_text_tokenizer, SmartCollator\n",
    "from context_enforcement.trainers.common import get_dataset_specified_tasks\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "seed_everything(1376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.ones((1,2),dtype=torch.long,device='cuda')\n",
    "z.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(z).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs[\"model_base\"] = \"facebook/bart-base\"\n",
    "configs[\"task_type\"] = \"xsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/home/nlplab/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d7f329eae14a8ea26a24b0d8c2a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = create_text_tokenizer(configs[\"model_base\"])\n",
    "\n",
    "task_dataset_gen = get_dataset_specified_tasks(configs[\"task_type\"])\n",
    "\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "test_dataset = None\n",
    "if task_dataset_gen is not None:\n",
    "    raw_dataset = task_dataset_gen(tokenizer=tokenizer, )\n",
    "    train_dataset = raw_dataset['train']\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    test_dataset = raw_dataset['test']\n",
    "\n",
    "model_builder = model_init(\n",
    "        vocab_size=len(train_dataset.tokenizer),\n",
    "        model_base=configs[\"model_base\"],\n",
    "        is_baseline=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = model_builder()\n",
    "train_model_path = \"trained_models_sum_boundary/bart_base_model_baseline/checkpoint-25506/pytorch_model.bin\"\n",
    "state_dict = torch.load(train_model_path)\n",
    "generator.load_state_dict(state_dict)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "te1= test_dataset[6]\n",
    "b_input_ids = te1.input_ids.view(1, -1).to(device)\n",
    "b_input_mask = te1.attention_mask.view(1, -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wendy Houvenaghel says British Cycling \"has acknowledged and takes very seriously previous cultural and governance failings\" in its World Class programme.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    bb=generator.generate(input_ids=b_input_ids[:,:1024],\n",
    "                attention_mask=b_input_mask[:,:1024],\n",
    "                eos_token_id=test_dataset.tokenizer.eos_token_id,\n",
    "        max_length=129,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        num_beams=5,\n",
    "        )\n",
    "test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A \"medal at any cost\" approach created a \"culture of fear\" at British Cycling, says former rider Wendy Houvenaghel.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.tokenizer.decode(te1.labels,clean_up_tokenization_spaces=True,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1187])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te1.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media playback is not supported on this device\n",
      "The Olympic silver medallist accused the organisation of \"ageism\" and having \"zero regard\" for her welfare.\n",
      "She is the latest high-profile cyclist to come forward after Jess Varnish, Nicole Cooke and Emma Pooley criticised the World Class programme.\n",
      "Houvenaghel told the BBC she felt \"vindicated\" by a leaked draft report detailing British Cycling's failures.\n",
      "The report said British Cycling \"sanitised\" its own investigation into claims former technical director Shane Sutton used sexist language towards Varnish, who went public last April about her treatment.\n",
      "British Cycling subsequently admitted it did not pay \"sufficient care and attention\" to the wellbeing of staff and athletes at the expense of winning medals, an approach Houvenaghel attested to in her BBC interview.\n",
      "Both Sutton and predecessor Sir Dave Brailsford have now left British Cycling.\n",
      "Houvenaghel, 42, spoke to BBC Sport during its State of Sport week, which on Thursday examines the issue of athlete welfare versus a win-at-all-costs culture.\n",
      "A government-commissioned review, headed by 11-time Paralympic champion Baroness Grey-Thompson, into safety and wellbeing in British sport, is due to be published imminently.\n",
      "It is expected to recommend significant reforms designed to improve the way athletes are treated by governing bodies.\n",
      "Houvenaghel claimed:\n",
      "British Cycling said it \"has acknowledged and takes very seriously previous cultural and governance failings in the World Class Programme\".\n",
      "It said it has accepted the draft report's findings and already put into a place a 39-point action plan to \"systematically address the cultural and behavioural shortcomings\".\n",
      "The statement added: \"Our new chair Jonathan Browning has apologised for instances where we have fallen short in our commitment to athlete welfare and has offered to meet with anyone who can help improve British Cycling.\"\n",
      "Who else has spoken out?\n",
      "Houvenaghel won silver in the individual pursuit at the Beijing Olympics in 2008, and gold in the World Championship team pursuit in 2008, 2009 and 2011.\n",
      "She retired in 2014, aged 39, after withdrawing from the Commonwealth Games in Glasgow with a back injury.\n",
      "Houvenaghel was critical of both Sutton and her team-mates in the aftermath of the London 2012 Olympics, where she was left out of all three team pursuit races as Dani King, Laura Trott and Joanna Rowsell-Shand won gold in a world record time.\n",
      "Speaking to BBC Sport this week, the Northern Irish rider said that experience was \"very traumatic\" and she felt \"torment\" at having \"no explanation\" for her last-minute omission.\n",
      "At the time, Brailsford, then performance director, defended the selection saying they had to \"take the personal element out of it, and look at the data and be professional\".\n",
      "He added: \"I think when a team steps up and makes six world records on the trot and a gold medal, then I don't think you can argue with that.\"\n",
      "British Cycling reiterated that point on Thursday, adding it was \"proud to support Wendy in what was a wonderfully successful cycling career\" and she was \"part of a pioneering generation of riders who set new standards of excellence\", but was dropped in London 2012 \"based on her performance\".\n",
      "Other elite cyclists, including King and Roswell-Shand have praised the leadership at British Cycling.\n",
      "Asked whether she was simply not good enough for the 2012 team, Houvenaghel replied: \"It was definitely not about performance. I don't think the fastest team on the day were permitted to race.\n",
      "\"There are certain chosen riders on the team who will not have experienced the culture of fear and will not have been on the receiving end of that - the bullying, the harassment, being frozen out of opportunities.\n",
      "\"It was horrid - it was not the training environment I expected. There was no choice. If you rocked the boat, you were out. There was no alternative.\n",
      "\"Medals at any cost, that's how it was whenever I was there, certainly in 2012.\"\n",
      "Houvenaghel said she also witnessed the sexism that has been highlighted by other female riders, and also claims she was discriminated against because of her age.\n",
      "\"I can certainly relate to the bullying,\" she said. \"For me personally, I felt it was more ageism - being a little bit older than my team-mates, it didn't seem to be something that the staff necessarily wanted for our team in 2012.\n",
      "\"They didn't care about what happened to me afterwards. I never heard another thing from them.\n",
      "\"After six years of constantly medalling at World Cups, World Championships, nationals, both on the track and on the road, they discarded me in a very undignified way from the team, which I don't feel was right.\"\n",
      "Fourteen-time Paralympic gold medallist Dame Sarah Storey told BBC Sport that elite level sport in Britain is \"cut-throat\" but there are \"no excuses for crossing that line\" into bullying.\n",
      "Asked about the balance between winning and athlete welfare, the 39-year-old replied: \"It's a really difficult question because you have to be a human being, you have to allow for people to make mistakes. But the currency is race wins, the currency is gold medals.\n",
      "\"It's not an excuse but you have to have a thick skin in sport, you have to be able to take the rough with the smooth because of the racing that you go through.\n",
      "\"But there are no excuses for crossing that line, and if those lines have been crossed they will be found out and they'll be dealt with.\"\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.tokenizer.decode(te1.input_ids,clean_up_tokenization_spaces=True,skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 610)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 945/945 [10:56<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "import tqdm\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=12,\n",
    "                              sampler= SequentialSampler(test_dataset),\n",
    "                              collate_fn= SmartCollator(\n",
    "            pad_token_id=train_dataset.tokenizer.pad_token_id, max_len=800,\n",
    "            context_max_len=configs.c,\n",
    "            context_sampling_bounds=(0.1, 0.45)\n",
    "        )\n",
    "                              )\n",
    "\n",
    "output_summaries2 =[]\n",
    "for batch in tqdm.tqdm(test_data_loader):\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    seq_len = b_input_ids.shape[1]\n",
    "    \n",
    "    boundary_mask =  batch.get(\"boundary\",)\n",
    "    bb=generator.generate(input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            num_beams=10,\n",
    "            do_sample=False,\n",
    "            num_return_sequences=1,\n",
    "            max_length=140)\n",
    "    sentences = test_dataset.tokenizer.batch_decode(bb,clean_up_tokenization_spaces=True,skip_special_tokens=True)\n",
    "    output_summaries2+=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 470)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#context_enforcement/data/common.py\n",
    "from context_enforcement.data.common import write_to_file\n",
    "write_to_file(output_summaries2[:len(test_dataset)], \"outputs/attempt1/baseline_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [tokenizer.decode(c.labels,clean_up_tokenization_spaces=True,skip_special_tokens=True) for c in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Musician Koffi Olomide has been taken into custody in the Democratic Republic of Congo, days after he was deported from Kenya for allegedly kicking one of his dancers at an airport in Nairobi.',\n",
       " 'Kenyan authorities have arrested Congolese singer Emmanuel Olomide in the capital, Kinshasa, on suspicion of assault, the BBC understands.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[690],output_summaries2[690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nlplab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "metrics = evaluate.combine(['bleu','meteor',\"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores4 = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = metrics.compute(predictions=output_summaries2,references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.13001522827171397,\n",
       " 'precisions': [0.43264780948765,\n",
       "  0.17764752271710285,\n",
       "  0.09726789720912225,\n",
       "  0.057229256856921935],\n",
       " 'brevity_penalty': 0.90401138133862,\n",
       " 'length_ratio': 0.9083367181931746,\n",
       " 'translation_length': 237045,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.3623311725177827,\n",
       " 'rouge1': 0.40646161409356885,\n",
       " 'rouge2': 0.18746257090958113,\n",
       " 'rougeL': 0.3322105156464427,\n",
       " 'rougeLsum': 0.33234181119671774}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.08190144303338862,\n",
       " 'precisions': [0.39916558741021463,\n",
       "  0.13484576712224944,\n",
       "  0.06490478666102122,\n",
       "  0.03461400195153127],\n",
       " 'brevity_penalty': 0.7810193894484071,\n",
       " 'length_ratio': 0.8018247587808374,\n",
       " 'translation_length': 209249,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.28558238733460617,\n",
       " 'rouge1': 0.34475899874216664,\n",
       " 'rouge2': 0.13444853158452552,\n",
       " 'rougeL': 0.279360220097055,\n",
       " 'rougeLsum': 0.27932587220115546}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.08004077243766287,\n",
       " 'precisions': [0.39668147524032016,\n",
       "  0.1326978952590178,\n",
       "  0.06327462141553002,\n",
       "  0.03340461786535229],\n",
       " 'brevity_penalty': 0.7793378458842136,\n",
       " 'length_ratio': 0.8004414368155238,\n",
       " 'translation_length': 208888,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.28278544337874784,\n",
       " 'rouge1': 0.34230456443341767,\n",
       " 'rouge2': 0.13258619227287083,\n",
       " 'rougeL': 0.27739509890876,\n",
       " 'rougeLsum': 0.2774115466752346}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11312982003210258,\n",
       " 'precisions': [0.43367935409457903,\n",
       "  0.170965703452634,\n",
       "  0.09041255378385218,\n",
       "  0.051220088499377064],\n",
       " 'brevity_penalty': 0.8310753377221631,\n",
       " 'length_ratio': 0.8438570541756397,\n",
       " 'translation_length': 220218,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.3379248069191188,\n",
       " 'rouge1': 0.3902876664001791,\n",
       " 'rouge2': 0.17375564911724187,\n",
       " 'rougeL': 0.3204999917403623,\n",
       " 'rougeLsum': 0.3205687202163906}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.11146082917260128,\n",
       " 'precisions': [0.4317755562839142,\n",
       "  0.1685194251648763,\n",
       "  0.08904895332074475,\n",
       "  0.050525125222168364],\n",
       " 'brevity_penalty': 0.8286310630214466,\n",
       " 'length_ratio': 0.8417648276020631,\n",
       " 'translation_length': 219672,\n",
       " 'reference_length': 260966,\n",
       " 'meteor': 0.334512237524537,\n",
       " 'rouge1': 0.3879467178046803,\n",
       " 'rouge2': 0.17119907794549927,\n",
       " 'rougeL': 0.31801217096792334,\n",
       " 'rougeLsum': 0.31809606706132665}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
